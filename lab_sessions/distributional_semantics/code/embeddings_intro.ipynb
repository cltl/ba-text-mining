{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this notebook, we will show you:\n",
    "\n",
    "* How to load a distributional model in python (and where to download distributional models from)\n",
    "* How to train your own Word2Vec model (simple version) and a pointer for training your own distributional model with control over hyperparameters andthe option to compare unstable vactors (hyperwords code)\n",
    "* How to get insights into the quality and content of the distributional word representations using:\n",
    "    \n",
    "    (a) Simple cosine similarity operations\n",
    "    \n",
    "    (c) Running standard evaluation\n",
    "    \n",
    "    (b) Clustering \n",
    "    \n",
    "    \n",
    "    \n",
    "* How to run standard evaluations\n",
    "\n",
    "In addition, the notebook contains a small evercise for getting started on Dutch data.\n",
    "    \n",
    "\n",
    "**About this notebook:**\n",
    "\n",
    "This notebook is using python 3.6. It is recommeded to run it using Anaconda (which includes most packages used here). \n",
    "\n",
    "Even though you can deal with embeddings using commonly used libraries such as numpy, the Gensim library is a very easy way of getting a first impression: https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "\n",
    "I recommed installing it via pip:\n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "(Alternatively, you can use 'commandline magic' to install packages directly from the notebook as shown in the cell below.) \n",
    "\n",
    "In addition, this notebook uses:\n",
    "\n",
    "* NLTK (Natural language processing toolkit)\n",
    "* Spacy (optional)\n",
    "* Numpy \n",
    "* Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can use pip install from within the notebook like this:\n",
    "\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Downloading or creating a distributional semantic model \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a Links for downloading existing models\n",
    "\n",
    "Follow the links to brows available models. The sources listed below contain English models trained using different algorithms, data with different degrees of preprocessing and varying hyperparameter settings. Some resources also include models in other languages (even Dutch with a bit of luck). \n",
    "\n",
    "### Large and commonly used models (English):\n",
    "\n",
    "* Google word2vec: o be downloaded from here (follow link in instructions): http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "* GloVe (trained on various corpora): https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "* FastText embeddings (Facebook): https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "* Models with different algorithms, hyperparamtersdimensions and degrees of preprocessing (e.g. dependency parsing windows):  https://vecto.readthedocs.io/en/docs/tutorial/getting_vectors.html\n",
    "\n",
    "\n",
    "\n",
    "### Various models in English & other languages:\n",
    "\n",
    "* word2vec trained on Wikipedia for various languages (including Dutch): https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "\n",
    "* Various algorithms and parameters for English and other languages: http://vectors.nlpl.eu/repository/#\n",
    "\n",
    "* Word2vec wikipedia for English and German: https://github.com/idio/wiki2vec\n",
    "\n",
    "* fastText for languages other than English: https://fasttext.cc/docs/en/crawl-vectors.html \n",
    "\n",
    "\n",
    "[TO DO: turn into table with speifications: Language, underying corpus, underlying corpus size, algorithm, hyperparameters]\n",
    "\n",
    "\n",
    "Gensim even lets you download models directly via their api. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b Creating your own model - the quick out-of-the box way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Required installations\n",
    "\n",
    "* requests: used for downloading data - you can download data manually instead\n",
    "* Gensim: Word2vec implementation for python\n",
    "* An NLP package for preprocessing: The examples here use NLTK ([Natural language processing toolkit](http://www.nltk.org/install.html). Alternatively, you can use [SpaCy](https://spacy.io/usage/models). If this is your first time using nltk, please make sure to download the most important corpora and resources (included in 'nltk book') but running `nltk.download()` (after having imported nltk). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can use pip install from within the notebook like this:\n",
    "\n",
    "# add/remove packages you (don't) need to install here\n",
    "%pip install requests\n",
    "#%pip install nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading nltk (only run this is you haven't done this already)\n",
    "import nltk \n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0: Download a corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a text corpus using python:\n",
    "# You can also do this manually by following this link: http://www.gutenberg.org/cache/epub/730/pg730.txt\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# project Gutenberg Oliver Twist as a .txt file:\n",
    "url = 'http://www.gutenberg.org/cache/epub/730/pg730.txt'\n",
    "r = requests.get(url)\n",
    "# Access content and decode bytes to utf-8\n",
    "text = r.content.decode('utf-8')\n",
    "\n",
    "\n",
    "# create directory for data\n",
    "if not os.path.isdir('../data/'):\n",
    "    os.mkdir('../data')\n",
    "\n",
    "# Write the text to a file and store it in our data directory (or do this step manually)\n",
    "with open('../data/oliver_twist.txt', 'w') as outfile:\n",
    "    outfile.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Preprocess the text**\n",
    "    \n",
    " There are several choices you can make in the preprocessing step. In general, you want to remove everything from the data that may introduce artifacts. You can also consider further regularization steps, such as replacing all numeric characters by the same representation or using lower case spelling for the entire corpus. ATTENTION: There is a trade-off between generalization and information (e.g. Lower- and uppercase spelling can be a relevant distinction, consider apple vs Apple).\n",
    " \n",
    "For smaller corpora, you will most likely want to remove punctuation and perhaps include some more regularization. You can even consider lemmatizing the text. If you inspect larger models (e.g. Google word2vec), you will notice that the vocabulary contains punctiuation (and all kinds of other weird symbols). For such a large dataset, the noise introduced by these things can most likely be neglected. \n",
    "\n",
    " Here, we do the following:\n",
    " \n",
    " * remove punctuation\n",
    " * set everythin to lower case\n",
    " * cut the text in sentences, so it can be processed by the vanilla, out of the box word2vec implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punct(tokens):\n",
    "    \n",
    "    # put to lower case \n",
    "    punct = string.punctuation \n",
    "    # Iterate over punctuation marks and replace them by an empty string one by one:\n",
    "    tokens_clean = []\n",
    "    for t in tokens:\n",
    "        if t not in punct:\n",
    "            tokens_clean.append(t)\n",
    "            \n",
    "    return tokens_clean\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    clean_sentences = []\n",
    "    sentences = sent_tokenize(text.strip())\n",
    "\n",
    "    for s in sentences:\n",
    "        tokens = word_tokenize(s.lower())\n",
    "        tokens_clean = remove_punct(tokens)\n",
    "        clean_sentences.append(tokens_clean)\n",
    "    return clean_sentences\n",
    "    \n",
    "test = \"This is a test text. Let's see if this works! TEST.\"\n",
    "\n",
    "preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to your text corpus \n",
    "\n",
    "# load data:\n",
    "\n",
    "with open('../data/oliver_twist.txt') as infile:\n",
    "    text_oliver = infile.read()\n",
    "\n",
    "# clean: \n",
    "text_oliver_clean = preprocess(text_oliver)\n",
    "print(text_oliver_clean[201:202])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Create word2vec model**\n",
    "\n",
    "Gensim allows you to train your own model. Here, we use a toy example, which should run on your local machine. If you'd like to train a larger corpus, you will most likely need to use a server.\n",
    "\n",
    "In the cell below, we train a model on the oliver twist novel you've downloaded in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# create directory for models\n",
    "if not os.path.isdir('../models/'):\n",
    "    os.mkdir('../models')\n",
    "\n",
    "oliver_w2v = Word2Vec(text_oliver_clean, size = 300, window = 4, min_count =2)\n",
    "# How to write out the model as a text file (vectors can be inspected easily)\n",
    "oliver_w2v.wv.save_word2vec_format('../models/oliver.txt')\n",
    "# How to write out the model as a binary file (cannot be inspected easily)\n",
    "oliver_w2v.wv.save_word2vec_format('../models/oliver.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we train a model on the movie reviews corpus included in nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "mr = Word2Vec(movie_reviews.sents())\n",
    "mr.wv.save_word2vec_format('../models/movies.bin', binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.c Creating your own model with control over hyperparameters \n",
    "\n",
    "If you want to create your own embeddings, this repository is a good start: \n",
    "\n",
    "https://bitbucket.org/omerlevy/hyperwords/src/default/\n",
    "\n",
    "Attention: code requires python2. We're woring on a python 3 version. \n",
    "\n",
    "**This is optional and probably takes quite some.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Accessing word representations of different models\n",
    "\n",
    "\n",
    "Models may be stored in different (and sometimes a little confusing) formats, but they all boil down to these components:\n",
    "\n",
    "* a matrix of word vectors \n",
    "* a vocabulary\n",
    "* a mapping between vectors in the matrix to the words in the vocabulary (often via indices)\n",
    "\n",
    "Even though there is existing software for inspecting and manipulating vectors (e.g. in the Gensim Word2vec toolkit), you can easily write code yourself using numpy (my preferred way of working). This way, you don't have to rely on non-transparent implementations (remember the analogy example...). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a Accessing models using the Word2vec toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load a stored model:\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#oliver_w2v = KeyedVectors.load_word2vec_format('../models/oliver.txt', binary=False) \n",
    "oliver_w2v = KeyedVectors.load_word2vec_format('../models/oliver.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the word2vec model as a python object:\n",
    "print('The model is represented internally as a...')\n",
    "print(type(oliver_w2v))\n",
    "print()\n",
    "#####\n",
    "vocabulary = oliver_w2v.vocab\n",
    "print('The model vocabulary is represented internally as a...')\n",
    "print(type(vocabulary))\n",
    "print('Some words from the model vocabulary:')\n",
    "print(list(vocabulary.keys())[:20])\n",
    "print('Information stored in the vocabulary for a word:')\n",
    "print(vocabulary['man'])\n",
    "print()\n",
    "#####\n",
    "# To access the vector of a particular word, you can simply do the following:\n",
    "vec_word = oliver_w2v['day']\n",
    "# This way, you access the vector as a numpy array\n",
    "print('Representation of an individual word vector:')\n",
    "print(type(vec_word))\n",
    "print('Number of vector dimensions:', len(vec_word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b Accessing a model without a specific package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can write your own code for loading your model as a numpy matrix. \n",
    "# I suggest to do this\n",
    "import numpy as np\n",
    "\n",
    "def load_model(path):\n",
    "    \n",
    "    matrix = []\n",
    "    vocab = []\n",
    "    word2index_dict = dict()\n",
    "    \n",
    "    with open(path) as infile:\n",
    "        lines = infile.read().split('\\n')\n",
    "        \n",
    "    for n, line in enumerate(lines[1:]):\n",
    "        line_list = line.split(' ')\n",
    "        word = line_list[0]\n",
    "        vocab.append(word)\n",
    "        vec = [float(v) for v in line_list[1:]]\n",
    "        matrix.append(vec)\n",
    "        word2index_dict[word] = n\n",
    "        \n",
    "    return np.array(matrix), vocab, word2index_dict\n",
    "        \n",
    "matrix, vocab, word2index_dict = load_model('../models/oliver.txt')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inspecting word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Time to explore what the model represents! Play around with similarity, nearest neighbors and analogies and try to get a feeling for what the vectors can do. Feel free to load existing models and compare what they represent. The code snippets below continue with the Oliver Twist toy example - so don't be disappointed if it returns nonsense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a Simple vector operations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load a stored model:\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "oliver_w2v = KeyedVectors.load_word2vec_format('../models/oliver.txt', binary=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity\n",
    "\n",
    "cos_man_woman = oliver_w2v.similarity('man', 'woman')\n",
    "cos_man_dog = oliver_w2v.similarity('man', 'dog')\n",
    "\n",
    "print(f'Man and woman should be more similar than man and dog:')\n",
    "if cos_man_woman > cos_man_dog:\n",
    "    print('True!')\n",
    "    print('man-woman', cos_man_woman)\n",
    "    print('man-dog', cos_man_dog)\n",
    "else:\n",
    "    print('False')\n",
    "    print('man-woman', cos_man_woman)\n",
    "    print('man-dog', cos_man_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nearest neighbors \n",
    "\n",
    "# Tip: use the help function if you want to explore the arguments\n",
    "#help(oliver_w2v.most_similar)\n",
    "nearest_neighbors = oliver_w2v.most_similar('dog', topn=10)\n",
    "for w, cos in nearest_neighbors:\n",
    "    print(w, cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analogy\n",
    "\n",
    "closest_to_predicted_vec = oliver_w2v.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)\n",
    "\n",
    "for word, cosine in closest_to_predicted_vec:\n",
    "    print(word, cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numpy (generally applicable)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cosine similarity is one of the most important concepts to understand if you want to work with vectors. It is calculated as shown below:\n",
    "\n",
    "![Cosine similarity](../images/cosine.png \"Logo Title Text 1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "# Using numpy:\n",
    "\n",
    "def normalize_vector(vec):\n",
    "   \n",
    "    # magnitude of the vector\n",
    "    mag = math.sqrt(sum([pow(value, 2) for value in vec]))\n",
    "\n",
    "    unit_vec = []\n",
    "\n",
    "    for value in vec:\n",
    "        unit_vec.append(value/mag)\n",
    "    unit_vec = np.array(unit_vec)\n",
    "    \n",
    "    \n",
    "def get_cosine(vec1, vec2):\n",
    "\n",
    "    vec1_norm = normalize_vector(vec1)\n",
    "    vec2_norm = normalize_vector(vec2)\n",
    "\n",
    "    cos = np.dot(vec1_norm, vec2_norm)\n",
    "\n",
    "    return cos\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.b  Standard evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluations sets can be found here:\n",
    "\n",
    "Similarity\n",
    "\n",
    "* WordSim 353: http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/\n",
    "* SimLex 999: https://fh295.github.io/simlex.html\n",
    "* MEN: https://staff.fnwi.uva.nl/e.bruni/MEN\n",
    "* Luong rare words: http://www.bigdatalab.ac.cn/benchmark/bm/dd?data=Rare%20Word\n",
    "\n",
    "\n",
    "\n",
    "Analogy \n",
    "\n",
    "* Google test sets (combined): http://download.tensorflow.org/data/questions-words.txt\n",
    "* Google test sets (semantic and morphological)https://bitbucket.org/omerlevy/hyperwords/src/default/testsets/analogy/\n",
    "* BATS: http://vecto.space/projects/BATS/\n",
    "\n",
    "\n",
    "Gensim already contains functions to run some standard evaluations. ATTENTION: If you already have a Gensim version installed, make sure to update it. (Mine was out of date and the evaluations did not run.) \n",
    "\n",
    "**Discussion question: How can we compare the scores of the similarity and relatedness evaluations? How would you test whether the correlation between the model and human judgments of one model is better than the correlation between model and human judgments of another model?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gensim has the evaluation methods built in\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# no access to actual pairs\n",
    "# if you want to read up on the details, run:\n",
    "#help(oliver_w2v.evaluate_word_pairs)\n",
    "pearson, spearman, oov = oliver_w2v.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "\n",
    "print('Pearson score', pearson)\n",
    "print('Spearman Rho score', spearman)\n",
    "print('Proportion of out ov vocabulary words', oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogy evaluation = actually gives you the model output sorted into correct and \n",
    "# incorrect predictions \n",
    "score, output= oliver_w2v.evaluate_word_analogies(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.c Clustering \n",
    "\n",
    "\n",
    "A nice way of inspecting word vectors is testing how they behave in clustering. Scikit learn offers a number of implementations of different [clustering algorithms](https://scikit-learn.org/stable/modules/clustering.html). \n",
    "\n",
    "\n",
    "Note: This is just to get a first impression. I recommed reading up on clustering evaluation for using larger sets without label annotations. Scikit learn is a good start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = ['apple', 'orange', 'lemon', 'strawberry', 'tomato']\n",
    "vegetables = ['cucumber', 'pepper', 'carrot', 'zucchini', 'egg_plant']\n",
    "animals = ['cat', 'dog', 'chicken', 'shrimp', 'lion', 'hamster', 'jaguar']\n",
    "abstract_concepts = ['feeling', 'idea', 'thought', 'theory', 'anger', 'aggression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def map_words(words, label, word_label_dict):\n",
    "    for word in words:\n",
    "        word_label_dict[word] = label\n",
    "\n",
    "word_label_dict = dict()\n",
    "map_words(animals, 'animal', word_label_dict)\n",
    "map_words(fruits, 'fruit', word_label_dict)\n",
    "map_words(vegetables, 'vegetable', word_label_dict)\n",
    "map_words(abstract_concepts, 'abstract', word_label_dict)\n",
    "\n",
    "for label, words in word_label_dict.items():\n",
    "    print(label, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_vectors(word_label_dict, model):\n",
    "    \n",
    "    vecs = []\n",
    "    words_in_vocab = []\n",
    "    \n",
    "    for word in word_label_dict.keys():\n",
    "        if word in model.vocab:\n",
    "            vec = model[word]\n",
    "            vecs.append(vec)\n",
    "            words_in_vocab.append(word)\n",
    "        else:\n",
    "            print(word, 'oov')\n",
    "    \n",
    "    return np.array(vecs), words_in_vocab\n",
    "\n",
    "\n",
    "vecs, words_in_vocab = get_all_vectors(word_label_dict, oliver_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering doc: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n",
    "\n",
    "# as many clusters as classes:\n",
    "\n",
    "n_clusters = len(set(word_label_dict[word] for word in words_in_vocab))\n",
    "print('number of clusters', n_clusters)\n",
    "# abstract vs concrete?\n",
    "#n_clusters = 2\n",
    "y_pred = KMeans(n_clusters=n_clusters, init='random').fit_predict(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clusters = defaultdict(list)\n",
    "for word, pred_label in zip(words_in_vocab, y_pred):\n",
    "    predicted_clusters[pred_label].append(word)\n",
    "    \n",
    "for label, words in predicted_clusters.items():\n",
    "    print(label, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise for Dutch\n",
    "\n",
    "Spacy for Dutch: https://spacy.io/models/nl\n",
    "\n",
    "Spacy quickstart: https://spacy.io/usage\n",
    "\n",
    "1) Find a Dutch corpus\n",
    "\n",
    "2) Preprocess it using Spacy for Dutch\n",
    "\n",
    "3) Create a model (using Gensim or something else)\n",
    "\n",
    "4) See if you can get an impression of what it captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the Dutch models, please run the following line from the terminal. Make sure that the command `python` is linked to the same python version used by anaconda. You can use `python --version` to find out. \n",
    "\n",
    "`python -m spacy download nl_core_news_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "doc = nlp(u\"Dit is een zin.\")\n",
    "# Accessing tokens:\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
