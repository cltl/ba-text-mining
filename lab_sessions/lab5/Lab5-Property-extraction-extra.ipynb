{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5-Property extraction (extra assignment)\n",
    "\n",
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "\n",
    "In this notebook, we provide more information about the task of Property Extraction.\n",
    "\n",
    "Overview of the content covered in this notebook:\n",
    "1. Introduction to property extraction\n",
    "2. Building pattern-based extractors\n",
    "3. Coding pattern-based extractors\n",
    "4. Processing Wikipedia documents\n",
    "5. Evaluating extractors\n",
    "\n",
    "**At the end of this notebook, you will be able to**:\n",
    "* understand the task of Property Extraction and its relation to similar tasks, like Relation Extraction\n",
    "* understand the parts of a property extraction tool\n",
    "* build a pattern-based Property Extractor based on substring matching\n",
    "* build a pattern-based Property Extractor based on dependency parsing information\n",
    "* apply an extractor to extract properties from text\n",
    "* apply extractors on Wikipedia\n",
    "* evaluate a pattern-based extractor\n",
    "\n",
    "Throughout this notebook we use the terms attribute, property and relation to mean the same thing a \"predicate\" in a triple relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we start: set up your environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Install Wikipedia client** In this week's lab session we are going to use Wikipedia. You first need to install a client package to access Wikipedia. From the terminal (with the settings that you use for notebooks) run:\n",
    "\n",
    "`conda install -c conda-forge wikipedia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Internet** Note that you need to have Internet connection to be able to access Wikipedia. If you are not connected or the connection is too slow you get the following error:\n",
    "\n",
    "`NewConnectionError: <urllib3.connection.HTTPConnection object at 0x1289d3c88>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known`\n",
    "\n",
    "**3. SpaCy** Another library that we will use in this week's lab sesion is SpaCy and its English model \"en_core_web_sm\". You should have this already installed because we used this setup in assignment 3. Let's import SpaCy and its English model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Loaded model 'en_core_web_sm'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "model=\"en_core_web_sm\"\n",
    "\n",
    "nlp = spacy.load(model)\n",
    "print(\"Info: Loaded model '%s'\" % model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. NLTK** NLTK is a standard language processing module in Python. This should be installed by defult with conda, but in case this is not the case, you can install it with:\n",
    "\n",
    "`conda install -c anaconda nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tree module is used in the lab5_utils.py file, where auxiliary classes are given to extract properties from text. We discuss these in more detail below. The above import is also done in lab5_utils.\n",
    "\n",
    "If the above code blocks did not yield any error, then you are all set up for this week's session. Let's start ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to property extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the task of entity linking (see Lab4), we performed disambiguation of entity mentions in text by making a connection to the correct referrent for a mention in a knowledge base. Although these knowledge bases are typically fairly large, they are far from complete. Tasks like property extraction and relation extraction help to make knowledge bases more complete, by extracting knowledge from text.\n",
    "\n",
    "The task of property extraction aims to fill knowledge bases with information about properties of entities that we find in text. There are other tasks that are similar to it, such as: \n",
    "* slot filling, where we attempt to complete entity information according to some schema\n",
    "* relation extraction - given two entities, what is their relation (for example, in Microsoft X Bill_Gates, the relation X is `hasCEO`)\n",
    "* knowledge base completion, where we usually complete a knowledge base by inference from existing structured information (not from text).\n",
    "* open information extraction - no schema available, very little restrictions on what to extract, disambiguation is non-trivial\n",
    "\n",
    "In all these tasks, including property extraction, we typically extract \"pieces of knowledge\" in the form of **a triple**. A triple consists of three elements: a subject, a predicate, and an object (not to be confused with grammatical functions in a syntactic dependency structure). An example of a triple is:\n",
    "\n",
    "```[Barack_Obama hasAge 57]```\n",
    "\n",
    "Here, Barack_Obama is a subject, hasAge is a predicate/relation, and 57 is an object. In the Semantic Web, the subjects and the predicates of a triple are always URIs; the objects can be either a URI (like Barack_Obama), or a literal (like 57, or \"Barack\").\n",
    "\n",
    "Hence, property extraction typically requires us to:\n",
    "1. **detect** a property value in text (e.g., 57) and an entity it belongs to (\"Barack\")\n",
    "2. **interpret** both the property value (57 as a number) and the entity (\"Barack\" means Barack_Obama)\n",
    "3. **find their relation** - the connection between Barack_Obama and 57 is the relation hasAge\n",
    "\n",
    "In this sense, the task of property extraction builds on top of the output of NERC and NERD.\n",
    "\n",
    "**Challenges** The property extraction task is difficult for reasons similar to those we have discussed with entity linking: ambiguity (of entities, relations, and property values), variation (of entities, values, and relations), and vagueness (when insufficient details or information is given). Futhermore,  relations can sometimes span multiple sentences or require a lot of world knowledge in order to understand them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building extractors\n",
    "\n",
    "The main focus of this week's lab session is on creating our own property extractors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Methods\n",
    "Building automatic property extractors is not trivial, because it requires multiple steps, and these steps in practice might differ a lot across different properties.\n",
    "\n",
    "There are two common methods for extracting properties from text: pattern matching and distant supervision (supervised machine learning). \n",
    "* The most basic approach for extracting properties from text is by pattern matching. This approach is transparent, but it requires us to define the patterns for each of the properties separately. For example, for the property \"birthplace\", we can use the pattern: \"X, born in Y\", or \"X from Y\". Typically, the patterns are combined with information on entity types to help their precision. For example, we will check whether indeed X is a person and Y is indeed a location in the above example. We can also filter syntactic depedencies, e.g. X should be the syntactic subject of the predicate \"born in\".\n",
    "* The second approach, distant supervision, relies on knowledge base information that is loosely based on a text. For example, you can think of a Wikipedia document that describes Donald Trump on the one hand, and structured information from Wikidata that tells us that he is born in 1946. However, we don't know if this information is explicitly mentioned in the Wikipedia text and if so, where and how. With the distant supervision method, we train for example a recurrent neural network on top of this kind of output, and hope that the neural network will learn the patterns in which this properperty is typically given in text.\n",
    "\n",
    "We will use a pattern matching approach to build our extractors in this week's lab session. If you are curious about how to build a distant supervision extractor, you can check Snorkel (their [introductory notebooks](https://github.com/HazyResearch/snorkel/tree/master) are quite user-friendly).\n",
    "\n",
    "#### 2.2 Building a pattern-based extractor\n",
    "\n",
    "Typically, a pattern-based extractor consists of several parts:\n",
    "1. find a mention of a specific property (for example, money or birth date) in text\n",
    "2. assign this mention to some subject entity\n",
    "3. normalize the property value\n",
    "4. normalize the subject entity\n",
    "\n",
    "**Example** Let's say we want to extract values for `founding year` in the following paragraph from Wikipedia:\n",
    "\n",
    "\"Juventus F.C. is an Italian professional football club based in Turin. The club was founded in March 1897 by a group of Torinese students.\"\n",
    "\n",
    "**Step I** First, we need to find property values that contain information about founding years. For example, we can use the pattern \"founded in\" to extract the property value `March 1897` in the second sentence.\n",
    "\n",
    "**Step II** Next, we need to see to which subject this property belongs. Assuming that we perform dependency parsing of the sentences, we can find that the relation \"founded in\" has a syntactic subject `The club`. At this point, we can extract the following relation:\n",
    "\n",
    "The club FOUNDING_YEAR March 1897\n",
    "\n",
    "Syntactially this is the correct way to extract the relation. However, the relation is not really very useful yet - we need to normalize its subject and object somehow to make it useful in a semantic sense.\n",
    "\n",
    "**Step III** Hence, we can normalize the value \"March 1897\" to a year value `1897`, for example, by looking for 4-digit numbers in the phrase.\n",
    "\n",
    "**Step IV** Then, we can normalize \"The club\" to `Juventus F.C.` by using entity coreference between the two sentences. We can then disambiguate the mention to https://en.wikipedia.org/wiki/Juventus_F.C., or `Juventus_F.C.` for brevity. This finally leads us to the following relation:\n",
    "\n",
    "`Juventus_F.C. FOUNDING_YEAR 1897`\n",
    "\n",
    "which looks much more useful (and it is on a semantic level, so we can store it in a knowledge base if we would like to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Coding pattern-based extractors\n",
    "\n",
    "**Implementing the four steps from 2.2** We will start by searching entity mentions of some type (e.g., nationality or date) by using SpaCy's named entity recognizer. \n",
    "\n",
    "Often this is not enough for step I, because an entity of a certain type can be a value of different properties. For example, the date \"1946\" can be a year of birth, a year of death, a founding year of a company, a year of starting/ending professional activity, etc. We will hence refer to this property value generation step as step Ia.\n",
    "\n",
    "In step Ib then, we can either:\n",
    "- check whether we find some keywords before the phrase (such as \"born in\" or \"founded in\"). We will build such an extractor in 3.1. \n",
    "- or use the dependency tree (also from SpaCy) to find the word that is associated with this value with a given dependency relation. We will do this in 3.2.\n",
    "\n",
    "For step II of assigning the value to some entity phrase, we will look for the closest entity to this property value and assign it to that one. Step III would mostly differ per kind of property value - e.g., it is different to normalize dates than it is to normalize company names. We will not deal with step IV in this week's session.\n",
    "\n",
    "**Helper functions** We will use a number of helper functions in 3.1 and 3.2. These functions are located in the file `lab5_utils.py`. You don't need to know their functionality in detail (if you are curious, feel free to check their code). Let's load these helper functions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lab5_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using substring matching\n",
    "We will use substring matching to look for founding years of organizations. \n",
    "We will use three simple patterns for this purpose: 'founded in X', 'established in X', 'created in X'. \n",
    "As mentioned before, we will also make sure that X is an entity of type DATE.\n",
    "\n",
    "You can find the full set of entity types recognized by SpaCy [here](https://spacy.io/usage/linguistic-features).\n",
    "\n",
    "We will use some helper functions from the file `lab5_utils.py` in this example:\n",
    "\n",
    "* `get_entities_of_type` to find all entities with a specific type - we will use this to find the property values in step Ia.\n",
    "* `pattern_found_on_the_left` tells us whether one of the patterns appears immediately on the left of the property value. This function internally calls the function `check_for_pattern` for each of the patterns. This is step Ib.\n",
    "* `find_closest_entity` to find the closest entity of type X to a property value (step II)\n",
    "* `extract_year_from_date` - given a full date, this function extracts a four-digit year number (step III).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the main function to extract date relations based on patterns. You can find the steps covered in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_relations(doc, patterns):\n",
    "    \"\"\"\n",
    "    Extract date properties from a spaCy document and assign them to an entity.\n",
    "    \"\"\"\n",
    "    \n",
    "    property_value_type='DATE'\n",
    "    target_entity_type='ORG'\n",
    "    \n",
    "    # the following 3 lines merge entities and noun chunks into one token\n",
    "    # The tokens \"New\" \"York\" will be retokenized into a single new token \"New York\"\n",
    "    # Check out the spaCy documentation for details\n",
    "    # The tokens in doc will be modified after calling this function\n",
    "    # this is useful in our cases, so we will always do it.\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge() ### will be replaced by the function \"retokenize\"\n",
    "    \n",
    "    relations = {}\n",
    "    \n",
    "    # step Ia - generate possible property values\n",
    "    dates=utils.get_entities_of_type(property_value_type, doc)\n",
    "    \n",
    "    for date in dates:\n",
    "        # step Ib - is one of our patterns found before the date \n",
    "        if utils.pattern_found_on_the_left(doc, date.i, patterns):\n",
    "            # step II - find the closest entity of some target type\n",
    "            org=utils.find_closest_entity(doc.ents, date.idx, target_entity_type)\n",
    "            # step III - normalize the year\n",
    "            year=utils.extract_year_from_date(date.text)\n",
    "            if year and org:\n",
    "                relations[org]=year\n",
    "\n",
    "    return relations        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test whether the founding date extraction works as we expect it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Airbus': 1991}\n"
     ]
    }
   ],
   "source": [
    "founded_patterns=['established in', 'founded in', 'created in']\n",
    "\n",
    "text='Airbus was founded in December 1991, not in January 1992.'\n",
    "\n",
    "doc = nlp(text)\n",
    "date_relations=extract_date_relations(doc, founded_patterns)\n",
    "print(date_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Using syntax dependencies\n",
    "\n",
    "\n",
    "Here, we extract organizations (entities labelled as `ORG`) and then check the dependency tree to find whether its syntax dependencies with other keywords (such as \"developed\") are as expected.\n",
    "\n",
    "In order to know which dependencies we need, let's first print the dependency tree of one example sentence and see which dependencies do we want to look for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         is_VBZ_ROOT                                                \n",
      "     _________________________|_____________                                         \n",
      "    |           |                    architecture_NN_                               \n",
      "    |           |                          attr                                     \n",
      "    |           |              _____________|________________                        \n",
      "    |           |             |                       developed_VBN_ac              \n",
      "    |           |             |                              l                      \n",
      "    |           |             |              ________________|_______________        \n",
      "    |           |             |             |                            in_IN_prep \n",
      "    |           |             |             |                                |       \n",
      "    |     88000_CD_nsubj      |        by_IN_agent                     1980s_CD_pobj\n",
      "    |           |             |             |                                |       \n",
      "._._punct   The_DT_det    an_DT_det  Motorola_NNP_pob                    the_DT_det \n",
      "                                            j                                       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"The 88000 is an architecture developed by Motorola in the 1980s.\"\n",
    "example_doc = nlp(u'' + text) \n",
    "\n",
    "for sent in example_doc.sents:\n",
    "    utils.to_nltk_tree(sent.root).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the nodes of the tree, we see three pieces of information: the word itself (\"Motorola\"), the part-of-speech tag (\"NNP\"), and the dependency tag (\"pobj\"). You can find an explanation of the dependency labels [here](https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md). \n",
    "\n",
    "In the example, we want to find cases where Motorola has a specific dependency relation with the word \"developed\". From the tree, you can see that we are looking for cases where our property value Motorola is a prepositional object (pobj) and the keyword we need (\"developed\") is two levels above Motorola (this is the head of the head of Motorola). We also see that this keyword has a dependency label \"acl\" (clausal modifier of a noun). \n",
    "\n",
    "Knowing this, we can now write a dependency function that searches for this kind of dependency patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting_dependency(token, predicates):\n",
    "    \"\"\"\n",
    "    Check whether the we find the right keyword in the correct part of the dependency tree.\n",
    "    \"\"\"\n",
    "    # Find prepositional objects that have a head with dependency label 'agent'\n",
    "    # and its head has a dependency label 'acl'\n",
    "    # Also, we make sure that the head of the head of our object is one of our keywords.\n",
    "    if token.dep_ == 'pobj' and token.head.dep_ == 'agent' and token.head.head.dep_ =='acl':\n",
    "        pred=token.head.head\n",
    "        if pred.text in predicates:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write our main function to extract device manufacturers from text. The helper functions we will use are:\n",
    "    \n",
    "- `get_entities_of_type` to find all entities with a specific type - we will use this to find the property values in step Ia.\n",
    "- `fitting_dependency` - to find whether any wanted keyword appears in the correct dependency relation to our property value (step Ib).\n",
    "- `find_closest_entity` to find the closest entity of type X to a property value (step II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_manufacturer(doc, predicates, main_entity):\n",
    "    \n",
    "    property_value_type='ORG'\n",
    "    target_entity_type='PRODUCT'\n",
    "    \n",
    "    # the following 3 lines merge entities and noun chunks into one token\n",
    "    # this is useful in our cases, so we will always do it.\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "    \n",
    "    relations={}\n",
    "    \n",
    "    # step Ia - generate possible property values\n",
    "    manus=utils.get_entities_of_type(property_value_type, doc)\n",
    "    \n",
    "    for manu in manus:\n",
    "        # step Ib - do we find the right keyword in the correct part of the dependency tree?\n",
    "        if fitting_dependency(manu, predicates):\n",
    "            # step II - find the closest entity of some target type\n",
    "            device=utils.find_closest_entity(doc.ents, manu.idx, target_entity_type)\n",
    "            # Devices are often not recognized properly by SpaCy - \n",
    "            # if we find no device, we assume that the relation is about the main entity of the document\n",
    "            if not device:\n",
    "                device=main_entity\n",
    "            if device and manu:\n",
    "                relations[device]=manu.text\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test whether the manufacturer extraction works as we expect it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Motorola 88000': 'Motorola'}\n"
     ]
    }
   ],
   "source": [
    "manu_predicates=['manufactured', 'produced', 'developed']\n",
    "text='The 88000 (m88k for short) is a RISC instruction set architecture (ISA) developed by Motorola during the 1980s.'\n",
    "main_entity='Motorola 88000'\n",
    "\n",
    "doc = nlp(text)\n",
    "manu_relations=extract_manufacturer(doc, manu_predicates, main_entity)\n",
    "print(manu_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "manu_predicates=['manufactured', 'produced', 'developed']\n",
    "\n",
    "good_text = 'Apple developed the 88000'\n",
    "bad_text = 'Apple is an American multinational technology company that designs, develops, and sells Apple TVs, Iphones and Ipads.'\n",
    "main_entity='Apple'\n",
    "doc = nlp(good_text)\n",
    "manu_relations=extract_manufacturer(doc, manu_predicates, main_entity)\n",
    "print(manu_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Running all our extractors\n",
    "\n",
    "Now we will write a function that runs all our extractors on a bunch of text documents paired with entities. The input is a dictionary consisting of the entity name as the key and the text sentence to find any of the relations as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_relations(texts):\n",
    "    \"\"\"\n",
    "    Run all our relations on a number of text documents.\n",
    "    \"\"\"\n",
    "    founding_years={}\n",
    "    manufacturers={}\n",
    "\n",
    "    founded_patterns=['established in', 'founded in', 'created in']\n",
    "    manu_predicates=['manufactured', 'produced', 'developed']\n",
    "\n",
    "    for main_entity, text in texts.items():\n",
    "        doc = nlp(text)\n",
    "        date_relations=extract_date_relations(doc, founded_patterns)\n",
    "        manu_relations=extract_manufacturer(doc, manu_predicates, main_entity)\n",
    "\n",
    "        founding_years.update(date_relations)\n",
    "        manufacturers.update(manu_relations)\n",
    "\n",
    "    return founding_years, manufacturers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Processing 2 texts\n",
      "\n",
      "**Extracted relations:**\n",
      "\n",
      "Founding years\n",
      "{'Airbus': 1991}\n",
      "Manufacturers\n",
      "{'Motorola 88000': 'Motorola'}\n"
     ]
    }
   ],
   "source": [
    "texts = {\n",
    "    'Motorola 88000': 'The 88000 (m88k for short) is a RISC instruction set architecture (ISA) developed by Motorola during the 1980s.',\n",
    "    'Airbus': 'Airbus was founded in December 1991, not in January 1992.',\n",
    "}\n",
    "\n",
    "print(\"Info: Processing %d texts\" % len(texts.keys()))\n",
    "print()\n",
    "print('**Extracted relations:**')\n",
    "print()\n",
    "\n",
    "founding_years, manufacturers=get_all_relations(texts)\n",
    "print('Founding years')\n",
    "print(founding_years)\n",
    "print('Manufacturers')\n",
    "print(manufacturers)\n",
    "    \n",
    "# Expected output:\n",
    "# {'Airbus': 1991}\n",
    "# {'Motorola 88000': Motorola}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Processing wikipedia\n",
    "\n",
    "Now that we know how to run our extractors on some text documents, we can do this on a larger scale. As an illustration, here we will load the first 3 sentences from a few Wikipedia documents and try to extract properties from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pacino\n",
      "Alfredo James Pacino (; Italian: [paˈtʃiːno]; born April 25, 1940) is an American actor and filmmaker In a career spanning over five decades, he has received many awards and nominations, including an Academy Award, two Tony Awards and two Primetime Emmy Awards He is one of the few performers to have received the Triple Crown of Acting\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_all_relations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-7ea6866cae5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msystem_founding_years\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem_manufactures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_relations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem_founding_years\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem_manufactures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_all_relations' is not defined"
     ]
    }
   ],
   "source": [
    "entities=[\"Motorola 88000\", \"Nokia\", \"The Coca-Cola Company\"]\n",
    "\n",
    "texts={}\n",
    "\n",
    "for entity in entities:\n",
    "    print(entity)\n",
    "    wp = wikipedia.page(entity)\n",
    "    # get the first 3 sentences of a wikipedia article\n",
    "    first_three_sentences=wp.content.split('.')[:3]\n",
    "    entity_text=('').join(first_three_sentences)\n",
    "    # create a dictionary (JSON) where the key is your entity, and the value is its 3-sentences wikipedia text. \n",
    "    texts[entity]=entity_text\n",
    "    print(entity_text)\n",
    "    print()\n",
    "    \n",
    "system_founding_years, system_manufactures = get_all_relations(texts)\n",
    "print(system_founding_years, system_manufactures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluating extractors\n",
    "\n",
    "We will evaluate the extractors by computing a precision, recall, and F1-score per document. We will only check the extracted values for the main entity in the document, not for any of the others.\n",
    "\n",
    "Similar as with entity linking, we will decide on true positives, false positives, false negatives per textual unit and not per class. In this case, the textual unit we will use is the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Evaluate founding years***\n",
      "precision: 0.000000, \n",
      "recall: 0.000000, \n",
      "F1-score: 0.000000\n",
      "\n",
      "***Evaluate manufacturers***\n",
      "precision: 1.000000, \n",
      "recall: 1.000000, \n",
      "F1-score: 1.000000\n"
     ]
    }
   ],
   "source": [
    "properties=['founding_year', 'nationality']\n",
    "gold={}\n",
    "gold_manufacturers={'Motorola 88000': 'Motorola'}\n",
    "gold_founding_years={'Nokia': 1865, 'The Coca-Cola Company': 1886}\n",
    "\n",
    "print('***Evaluate founding years***')\n",
    "precision, recall, f1=utils.evaluate_property(system_founding_years, gold_founding_years)\n",
    "print(\"precision: %f, \\nrecall: %f, \\nF1-score: %f\\n\" % (precision, recall, f1))\n",
    "\n",
    "print('***Evaluate manufacturers***')\n",
    "precision, recall, f1=utils.evaluate_property(system_manufactures, gold_manufacturers)\n",
    "print(\"precision: %f, \\nrecall: %f, \\nF1-score: %f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
