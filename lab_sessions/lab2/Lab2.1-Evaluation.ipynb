{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab2.1 Evaluation for classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most NLP tasks, e.g., part of speech tagging, sentiment analysis, etc., are defined as a classification task. The goal is to classify a textual unit, e.g., word, lemma, sentence, document, etc., into some category. In the case of sentiment analysis, the task is sometimes defined as classifying a textual unit such as *What a great hotel!* into one of three categories: *negative* | *neutral* | *positive*. It is essential that we evaluate NLP systems thoroughly, which gives us insight into its strengths and weaknesses. In this notebook, we show how to compute the most crucial evaluation metrics, which are Precision, Recall, Accuracy, and F<sub>1</sub>.\n",
    "\n",
    "**At the end of this notebook, you will be able to compute for n-class classification tasks**:\n",
    "* Precision (micro + macro)\n",
    "* Recall (micro + macro)\n",
    "* F<sub>1</sub> (micro + macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Please make sure you have the **sklearn** package installed (**conda install sklearn**, or **pip install sklearn**, or via **Anaconda Navigator**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: an example\n",
    "Natural Language is very complex. Each specific NLP task, therefore, only focuses on a small portion of this complexity. Hence, each NLP task has a **task definition**, which provides valuable information about how we should approach the task. What to categorize in Sentiment Analysis is typically a textual unit, e.g., a word, sentence, tweet, document, etc. Let's look at how Sentiment Analysis is often defined:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Task definition label  |  Categories |\n",
    "|---|---|\n",
    "| three-class  | *negative* or *neutral* or *positive*   | \n",
    "\n",
    "Examples:\n",
    "* **negative**: *What a horrible movie*\n",
    "* **neutral**: *Watermelons are 92% water, and since water is tasteless, you only ever taste 8% of a watermelon.*\n",
    "* **positive**: *I'm scared of reading another book since it will probably disappoint with respect to this one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites: Humans vs Computers\n",
    "The most common way to evaluate an NLP system is to compare what humans think to what computers think. We need to obtain data for which humans have annotated for Sentiment Analysis, which is often called **human annotation** or **gold**. Then, we let an NLP system perform Sentiment Analysis on the same data, which we will call **system_output**. For evaluation, we compare the **system_output** to the **gold**. We now show data for the *three-class* approach for Sentiment Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for three-class Sentiment Analysis\n",
    "Below, we show textual units, human annotation, and system output for the three-class approach for Sentiment Analysis. We will use this data later on in the notebook to compute the evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Textual unit | System label | Gold label | \n",
    "|---|---|---|\n",
    "| What a great movie! | pos | pos |\n",
    "| This movie was just amazing | pos | pos |\n",
    "| I want to watch this movie again | neg | pos |\n",
    "| This move will do well | neg | pos |\n",
    "| Fanastic movie | pos | pos |\n",
    "| What an awful movie | pos | neg |\n",
    "| Why was this movie funded at all? | neg | neg |\n",
    "| It was a move with persons | neu | neu |\n",
    "| Locations were shown in this movie | neu | neu |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The confusion matrix\n",
    "The next step is to build a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), in which we visualize the system performance for each category:  *negative* or *neutral* or *positive*. Each row represents the system labels. The columns represent the gold information. Please carefully inspect the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| - | Gold category negative | Gold category neutral | Gold category positive | - |\n",
    "|---|---|---|---|---|\n",
    "| System category negative | 1 | 0 | 2 | System predicted negative 3 times |\n",
    "| System category neutral | 0 | 2 | 0 | System predicted neutral 2 times |\n",
    "| System category positive | 1  | 0 | 3 | System predicted positive 4 times |\n",
    "| - | Negative occurs 2 times in gold | Neutral occurs 2 times in gold | Positive occurs 5 times in gold |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this table visualize? We'll explain a couple of cells to get you started:\n",
    "* [column 1, row 1, value: 1] This shows that the system correctly predicted the category negative once. \n",
    "* [column 1, row 3, value: 1] This shows that the system chose the category positive one time, but the gold category was actually negative.\n",
    "\n",
    "You already start to see that this gives us insight into the strengths and weaknesses of an NLP system. We begin to get an idea of which categories the system excels at, and at which it fails. The next step is to go from a table to quantifying system performance using evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably have heard of the Precision, Recall, and F<sub>1</sub>. All of these three evaluation metrics provide you with different insights:\n",
    "* **Precision**: when a system predicts a category, how accurate is it?\n",
    "* **Recall**: out of all the times a category occurred in the gold, how many times did the system predict it?\n",
    "* **F<sub>1</sub>**: harmonic mean between Precision and Recall\n",
    "\n",
    "Now comes an important part. Since we are dealing with multi-class classification, we have to consider the performance for **each category** before computing the overall Precision and Recall. We will use the following terminology:\n",
    "* Let **TP<sub>category</sub>** be the number of times a system correctly predicted a certain category (also called True Positives)\n",
    "* Let **N_gold<sub>category</sub>** be the number of times a certain category occurs in the gold data\n",
    "* Let **N_system<sub>category</sub>** be the number of times the system predicted a certain category\n",
    "\n",
    "We now can define Precision, Recall, and F<sub>1</sub> for a specific category:\n",
    "\n",
    "$$Precision_{category} = \\frac{TP_{category}}{N\\_system_{category}}$$\n",
    "\n",
    "$$Recall_{category} = \\frac{TP_{category}}{N\\_gold_{category}}$$\n",
    "\n",
    "$$F1_{category} = 2 * \\frac{Precision_{category} * Recall_{category}}{Precision_{category} + Recall_{category}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute this for the category *positive*. Please take a look at the same table as before \n",
    "\n",
    "| - | Gold category negative | Gold category neutral | Gold category positive | - |\n",
    "|---|---|---|---|---|\n",
    "| System category negative | 1 | 0 | 2 | System predicted negative 3 times |\n",
    "| System category neutral | 0 | 2 | 0 | System predicted neutral 2 times |\n",
    "| System category positive | 1  | 0 | 3 | System predicted positive 4 times |\n",
    "| - | Negative occurs 2 times in gold | Neural occurs 2 times in gold | Positive occurs 5 times in gold |\n",
    "\n",
    "* **TP<sub>category</sub>** is 3 since the system correctly predicted the positive category three times.\n",
    "* **N_gold<sub>category</sub>** is the sum of the values in the column *Gold category positive* (2 + 0 + 3 = 5)\n",
    "* **N_system<sub>category</sub>** is the sum of the values in the row *System category positive* (1 + 0 + 3 = 4)\n",
    "\n",
    "\n",
    "$$Precision_{positive} = \\frac{TP_{category}=3}{N\\_system_{category}=4} = 0.75$$\n",
    "\n",
    "$$Recall_{positive} = \\frac{TP_{category}=3}{N\\_gold_{category}=5} = 0.6$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards one number: the case of macro and micro\n",
    "Above, we show how to compute the Precision and Recall, and F<sub>1</sub> for one specific category. We would like to combine the output per category into one number. There are at least two ways of doing this:\n",
    "* **macro**: we aggregate the contributions at the category level, e.g., we average the Precision or Recall per category: negative, neutral, and positive.\n",
    "* **micro**: we aggregate the contributions at the textual unit level.\n",
    "\n",
    "Let **N_categories</sub>** be the number of categories in the data. In our case, this is 3 (negative, neutral, and positive).\n",
    "\n",
    "The definitions for our use case (three-class Sentiment Analysis) are as follows:\n",
    "\n",
    "$$Precision_{macro} = \\frac{Precision_{negative} + Precision_{neutral} + Precision_{positive}}{N_{categories}} $$\n",
    "\n",
    "$$Precision_{micro} = \\frac{TP_{negative} + TP_{neutral} + TP_{positive}}{N\\_system_{negative} + N\\_system_{neutral} + N\\_system_{positive}}$$\n",
    "\n",
    "$$Recall_{macro} = \\frac{Recall_{negative} + Recall_{neutral} + Recall_{positive}}{N_{categories}} $$\n",
    "\n",
    "$$Recall_{micro} = \\frac{TP_{negative} + TP_{neutral} + TP_{positive}}{N\\_gold_{negative} + N\\_gold_{neutral} + N\\_gold_{positive}}$$\n",
    "\n",
    "$$F1_{macro} = 2 * \\frac{Precision_{macro} * Recall_{macro}}{Precision_{macro} + Recall_{macro}}$$\n",
    "\n",
    "$$F1_{micro} = 2 * \\frac{Precision_{micro} * Recall_{micro}}{Precision_{micro} + Recall_{micro}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute evaluation metrics\n",
    "We will now compute the Precision, Recall, F<sub></sub> numbers for both macro and micro averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_class_human_annotation = [\n",
    "    \"pos\",\n",
    "    \"pos\",\n",
    "    \"pos\",\n",
    "    \"pos\",\n",
    "    \"pos\",\n",
    "    \"neg\",\n",
    "    \"neg\",\n",
    "    \"neu\",\n",
    "    \"neu\",\n",
    "]\n",
    "\n",
    "three_class_system_output = [\n",
    "    \"pos\", \n",
    "    \"pos\", \n",
    "    \"neg\",\n",
    "    \"neg\",\n",
    "    \"pos\",\n",
    "    \"pos\",\n",
    "    \"neg\",\n",
    "    \"neu\",\n",
    "    \"neu\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision<sub>macro</sub>\n",
    "$$Precision_{macro} = \\frac{Precision_{negative}=(1/3) + Precision_{neutral}=(2/2) + Precision_{positive}=(3/4)}{N_{categories}=3} = 0.69  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(y_true=three_class_human_annotation,\n",
    "                        y_pred=three_class_system_output,\n",
    "                        average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision<sub>micro</sub>\n",
    "$$Precision_{micro} = \\frac{TP_{negative}=1 + TP_{neutral}=2 + TP_{positive}=3}{N\\_system_{negative}=3 + N\\_system_{neutral}=2 + N\\_system_{positive}=4} = 0.67$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(y_true=three_class_human_annotation,\n",
    "                        y_pred=three_class_system_output,\n",
    "                        average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall<sub>macro</sub>\n",
    "$$Recall_{macro} = \\frac{Recall_{negative}=(1/2) + Recall_{neutral}=2/2) + Recall_{positive}=(3/5)}{N_{categories}=3} = 0.7 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall_score(y_true=three_class_human_annotation,\n",
    "                     y_pred=three_class_system_output,\n",
    "                     average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall<sub>micro</sub>\n",
    "$$Recall_{micro} = \\frac{TP_{negative}=1 + TP_{neutral}=2 + TP_{positive}=3}{N\\_gold_{negative}=2 + N\\_gold_{neutral}=2 + N\\_gold_{positive}=5} = 0.67$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall_score(y_true=three_class_human_annotation,\n",
    "                     y_pred=three_class_system_output,\n",
    "                     average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1<sub>macro</sub>\n",
    "$$F1_{macro} = 2 * \\frac{Precision_{macro}=0.69 * Recall_{macro}=0.70}{Precision_{macro}=0.69 + Recall_{macro}=0.70} = 0.69$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_true=three_class_human_annotation,\n",
    "                 y_pred=three_class_system_output,\n",
    "                 average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1<sub>micro</sub>\n",
    "$$F1_{micro} = 2 * \\frac{Precision_{micro}=0.67 * Recall_{micro}0.67}{Precision_{micro}=0.67 + Recall_{micro}=0.67} = 0.67$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_true=three_class_human_annotation,\n",
    "                 y_pred=three_class_system_output,\n",
    "                 average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn also has a method that will show all evaluation metrics in one classification report. For now, we do not consider *weighted avg*. You can ignore this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = metrics.classification_report(y_true=three_class_human_annotation,\n",
    "                                       y_pred=three_class_system_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric interpretation\n",
    "Now comes the hard part: interpretation. You've just computed a wealth of numbers! This part is up to you!\n",
    "Carefully inspect all the metrics and think about what this tells us about the strengts and weaknesses of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
