{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab.ML Machine learning using embeddings\n",
    "\n",
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "\n",
    "In this notebook, you are going to use word embeddings instead of the one-hot-encoding of words. Word embeddings have many advantages:\n",
    "\n",
    "* they capture similarities across words that can be learned from massive amounts of text data without annotation\n",
    "* machine learning can easily exploit similarity because the embeddings are also represented as vectors\n",
    "* the word embedding vectors are much smaller (100 up to 500 dimensions) and more dense than one-hot-encodings, which results in more efficient and compact models that also generalize better.\n",
    "\n",
    "At the end of this notebook, you should have learned:\n",
    "\n",
    "* how replace the words in your training set by there embeddings\n",
    "* how to train a classifier enriched with embeddings\n",
    "* how to represent the words for any unseen text as embeddings\n",
    "* how to add embeddings to our NERC system\n",
    "* how to work with some popular data sets for NERC with which such embeddings can be combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Quick introduction to embeddings\n",
    "\n",
    "Extracting features manually can get us a long way. In addition to lemma and part-of-speech, people have used other information: features of the previous words (on the left) or the next words (on the right), whether the current word starts with a capital, whether it is an abbreviation, etc.\n",
    "\n",
    "A recent alternative way to create a 'semantic' representation of a word is by word embeddings: mapping words (or phrases) from the vocabulary to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension. For this reason, they are called dense representations.\n",
    "\n",
    "In linguistics, word embeddings were discussed in the research area of distributional semantics. The idea is to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data. The underlying notion is that \"a word is characterized by the company it keeps\" (Firth). Embeddings are however the weights in the hidden layer of a neural network that is trained to predict the contexts rather than representing the context in a vector directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load pre-trained word embeddings called word2vec, created by Google. The embeddings have 300 dimensions.\n",
    "\n",
    "First, please download the file from [their google drive](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). Then, create a folder in the same directory as this notebook, called 'model' and unpack the word2vec file in that folder.\n",
    "\n",
    "We will load the embedding model with the Gensim package that we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the file using the gensim library (this takes a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings capture certain meaning aspects of words. Previous research has shown that they can partially capture simiarity (\"tapas\" is similar to \"pintxos\"), relatedness (tapas relates to Spain), and analogy (\"Paris\" is to \"France\" as \"Rome\" is to \"Italy\"). \n",
    "\n",
    "To get an idea of these properties of embeddings, we can compute the cosine similarity between two word vectors. We will expect for example, that \"cat\" and \"tiger\" are more similar than \"cat\" and \"Germany\". Feel free to play a bit with word1 and word2 below to get some feeling of the information these embeddings capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6477412]]\n"
     ]
    }
   ],
   "source": [
    "word1='tapas'\n",
    "word2='pintxos'\n",
    "word1_vector=np.array(word_embedding_model[word1]).reshape(1, -1)\n",
    "word2_vector=np.array(word_embedding_model[word2]).reshape(1, -1)\n",
    "print(cosine_similarity(word1_vector, word2_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the most similar words to some word, say 'apple':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apples', 0.7203598022460938), ('pear', 0.6450696587562561), ('fruit', 0.6410146355628967), ('berry', 0.6302294731140137), ('pears', 0.6133961081504822), ('strawberry', 0.6058261394500732), ('peach', 0.6025873422622681), ('potato', 0.596093475818634), ('grape', 0.5935864448547363), ('blueberry', 0.5866668224334717)]\n"
     ]
    }
   ],
   "source": [
    "print(word_embedding_model.most_similar('apple', topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Using embeddings in our NERC model\n",
    "\n",
    "Next, we will use the same example of Named Entity Recognition and Classification (NERC) as in the previous notebook but now replace the one-hot-vector for the vocabularies by their dense embeddings.\n",
    "\n",
    "We use the same text as before and process it using SpaCy to get the words and the part of speech. We define the labels in the same way as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text=\"Germany's representative to the European Union\"\n",
    "\n",
    "doc=nlp(text)\n",
    "\n",
    "## The series of labels that go with the word tokens from the input text\n",
    "y=['B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now replace the one-hot input representation of our words with embeddings. We generate our input data by simply looking up each word in the embeddings model. If we find it, we add the embedding vector to the training input, if not we add a vector with 300 zero values.\n",
    "\n",
    "The following code creates an array from all the tokens in the spaCy document object \"doc\" by taking the embedding vectors for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This word is not in the word2vec vocabulary: 's\n",
      "This word is not in the word2vec vocabulary: to\n"
     ]
    }
   ],
   "source": [
    "training_input=[]\n",
    "for token in doc:\n",
    "    word=token.text  #the next word from the tokenized text\n",
    "    # we check if our model (loaded with the Google word2vec embeddings)\n",
    "    # is inside the model\n",
    "    if word in word_embedding_model:\n",
    "        # in this case the word was found and vector is assigned with its embedding vector as the value\n",
    "        vector=word_embedding_model[word]\n",
    "    else: \n",
    "        # if the word does not exist in the embeddings vocabulary, \n",
    "        # we create a vector with all zeros.\n",
    "        # The Google word2vec model has 300 dimensions so we creat a vector with 300 zeros\n",
    "        vector=[0]*300\n",
    "        print('This word is not in the word2vec vocabulary:', word)\n",
    "    training_input.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for two tokens from the spaCy output, we did not get an embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the first element in our training_input, which is the same size as the tokenized sentence but the words are replaced by embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training input =  7\n",
      "[ 0.25976562  0.140625    0.24707031  0.00958252 -0.25       -0.08251953\n",
      " -0.09912109 -0.35351562 -0.1484375   0.1484375  -0.03540039 -0.05249023\n",
      "  0.09277344 -0.14257812 -0.01483154  0.01647949  0.03710938  0.18847656\n",
      " -0.03955078 -0.05786133  0.26757812  0.10693359 -0.04345703  0.06738281\n",
      " -0.00177765  0.1328125  -0.16308594 -0.05908203 -0.22558594  0.12207031\n",
      "  0.10791016 -0.19433594 -0.16210938 -0.14257812  0.09033203 -0.14648438\n",
      " -0.12109375  0.09960938  0.26367188  0.12695312  0.140625    0.11083984\n",
      "  0.02697754 -0.01635742  0.00292969  0.14746094 -0.06542969 -0.16699219\n",
      "  0.03662109  0.14941406 -0.14746094  0.06835938 -0.09228516  0.12207031\n",
      " -0.09179688  0.09082031 -0.38476562  0.03051758 -0.21679688 -0.12597656\n",
      " -0.08642578 -0.26171875 -0.08496094 -0.13964844 -0.02832031 -0.203125\n",
      "  0.29101562 -0.13574219 -0.07226562  0.16308594 -0.19042969  0.22265625\n",
      "  0.05566406  0.21289062  0.05053711 -0.09814453  0.12158203  0.01000977\n",
      "  0.15234375 -0.02233887  0.07324219 -0.27148438 -0.01977539 -0.07128906\n",
      "  0.13964844 -0.06542969  0.08886719  0.01452637  0.03344727  0.12158203\n",
      " -0.0703125  -0.02282715 -0.15332031  0.22265625 -0.22265625 -0.18164062\n",
      "  0.08642578 -0.26367188 -0.03735352  0.04321289  0.03039551 -0.19042969\n",
      "  0.07080078  0.37695312 -0.01318359 -0.02526855 -0.00775146  0.02868652\n",
      " -0.00350952 -0.08203125  0.15234375 -0.20996094  0.16796875  0.05322266\n",
      "  0.13476562  0.05102539  0.00454712  0.02099609  0.37890625  0.24707031\n",
      " -0.25585938 -0.27148438  0.0402832  -0.00604248 -0.11865234 -0.05957031\n",
      "  0.14550781 -0.265625   -0.47851562  0.02709961  0.03833008  0.02441406\n",
      " -0.1640625  -0.19921875 -0.08984375 -0.16015625 -0.06079102 -0.03491211\n",
      "  0.2109375   0.03637695  0.20898438 -0.00588989  0.015625    0.10986328\n",
      "  0.08154297  0.11328125  0.296875    0.30664062 -0.12890625 -0.14550781\n",
      "  0.11474609  0.02258301 -0.02905273 -0.08544922 -0.06201172 -0.08740234\n",
      " -0.21777344  0.10986328  0.14550781  0.21679688  0.07177734  0.02294922\n",
      "  0.13964844 -0.15917969  0.22265625 -0.03515625 -0.03515625  0.07763672\n",
      " -0.12695312  0.25       -0.24707031  0.04345703  0.05786133 -0.1171875\n",
      " -0.11425781 -0.29492188  0.11523438 -0.16992188 -0.05078125 -0.09228516\n",
      "  0.04101562 -0.3984375  -0.24707031 -0.19238281  0.4375      0.12988281\n",
      "  0.12890625  0.03271484 -0.22558594  0.20800781  0.03564453 -0.06982422\n",
      "  0.12792969 -0.00300598  0.17285156  0.15722656  0.12890625 -0.16210938\n",
      " -0.24121094 -0.3984375   0.02246094 -0.06176758  0.12792969  0.02526855\n",
      "  0.01867676 -0.01745605 -0.15722656 -0.06201172 -0.22460938 -0.02087402\n",
      "  0.00192261 -0.17382812  0.09521484  0.23046875 -0.16992188 -0.22363281\n",
      "  0.23632812  0.05126953  0.13867188 -0.00238037 -0.05664062 -0.2890625\n",
      " -0.11767578 -0.11767578 -0.28710938 -0.0324707   0.04150391 -0.08447266\n",
      "  0.1953125  -0.24121094 -0.19433594 -0.04418945  0.0559082   0.01293945\n",
      "  0.05444336  0.12988281 -0.02502441  0.19726562 -0.17675781 -0.15234375\n",
      "  0.21484375  0.00515747  0.01269531  0.11132812 -0.02722168  0.15625\n",
      " -0.20800781 -0.02416992 -0.11621094 -0.08789062 -0.02075195  0.01782227\n",
      "  0.00439453  0.18261719 -0.19433594  0.26367188  0.04467773  0.01293945\n",
      "  0.15917969  0.0859375   0.12988281  0.00141907  0.02441406  0.25976562\n",
      "  0.28320312  0.11669922 -0.03564453 -0.25       -0.0559082   0.29296875\n",
      "  0.04833984 -0.03393555  0.13769531 -0.19140625  0.21875    -0.08300781\n",
      " -0.10058594  0.20117188 -0.11425781 -0.00595093 -0.06982422 -0.046875\n",
      " -0.06591797  0.02453613 -0.27148438  0.19628906  0.22265625  0.02429199\n",
      "  0.04394531  0.10253906 -0.06079102  0.11816406 -0.06445312  0.11376953\n",
      " -0.17285156  0.171875   -0.19433594  0.00817871  0.06835938 -0.14160156]\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of the training input = \",len(training_input))\n",
    "#### the first token has the following embedding values\n",
    "print(training_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in the earlier cases, once we have the vector representations, we can use them to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(training_input, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the model** Let's say we want to test our model with the sentence: 'I love beer from Munich'. What we need to do is to preprocess the text in the same way as the training data by using spaCy (otherwise, we may get a mismatch in features), and next replace each word by an embedding vector as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence='I love beer from Munich'\n",
    "test_doc=nlp(test_sentence)\n",
    "\n",
    "test_input=[]\n",
    "\n",
    "for token in test_doc:\n",
    "    word=token.text\n",
    "    if word in word_embedding_model:\n",
    "        vector=word_embedding_model[word]\n",
    "    else:\n",
    "        vector=[0]*300\n",
    "    test_input.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our representation is the same, we can aske the classifier to make a prediction for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'O' 'O' 'O' 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "pred=lin_clf.predict(test_input)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier assigned IOB tags to the tokens in order and the final obtained the label 'B-LOC', which is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now trained and testing your first embeddings-based NERC model. Note that the word 'Munich' is not in the training data but the system still managed to make a correct(!) prediction because the embedding matched.\n",
    "\n",
    "So far you have just worked with a few toy examples. In order to obtain a good performance machine learning systems may need thousands and sometimes hundreds of thousands training examples. The vocabulary of a language is large and there is also large variation in expressions. Having only a few examples for each words or expression requires to have massive amounts of text.\n",
    "\n",
    "To some extent, word embedding resolve the issue of *data sparseness*, as words unseen in the training data may still be similar to other words that are in the training data. Word embeddings are derived from millions of documents (billions of tokens) and are likely to have embeddings for most words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Combining embeddings with one-hot encoding\n",
    "\n",
    "So how can we combine the word embeddings with one-hot-encodings for other features?\n",
    "\n",
    "We are first going to get the one-hot encondings of the text as we did in the previous notebook using the DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances=[]\n",
    "for token in doc:\n",
    "    one_training_instance={'part-of-speech': token.pos_, 'lemma': token.lemma_} # this concatenates the PoS and Lemma\n",
    "    training_instances.append(one_training_instance)\n",
    "\n",
    "the_array = vec.fit_transform(training_instances).toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the array, we see it holds 7 rows, each row representing one token, and 12 columns, each column representing a feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_array.shape\n",
    "# ROWS are WORDS, COLUMNS are FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# the first token values\n",
    "print(the_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(the_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training_input array represented the same text with embeddings. Let's inspect the array for the emddings using the numpy module (imported as np at the start of this notebook!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(training_input).shape\n",
    "# ROWS are WORDS, COLUMNS are EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has the same rows but 300 additional features. We can now *concatenate* the features for each word using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_input=np.array(the_array)\n",
    "embeddings_input=np.array(training_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(features_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the number of rows is the same across the two arrays and each row corresponds to the same token instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining the values for: 0  from the features and the embeddings\n",
      "Combining the values for: 1  from the features and the embeddings\n",
      "Combining the values for: 2  from the features and the embeddings\n",
      "Combining the values for: 3  from the features and the embeddings\n",
      "Combining the values for: 4  from the features and the embeddings\n",
      "Combining the values for: 5  from the features and the embeddings\n",
      "Combining the values for: 6  from the features and the embeddings\n"
     ]
    }
   ],
   "source": [
    "#### num_words is the number of rows\n",
    "num_words=features_input.shape[0]\n",
    "concat_input=[] # for storing the result of concatenating\n",
    "for index in range(num_words):\n",
    "    print('Combining the values for:', index, \" from the features and the embeddings\")\n",
    "    representation=list(features_input[index]) + list(embeddings_input[index]) # concatenate features per word\n",
    "    concat_input.append(representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the shape, we see it has the same rows but now the combination of features result in 312 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 312)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(concat_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect the concatenated vector for the first token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.259765625, 0.140625, 0.2470703125, 0.00958251953125, -0.25, -0.08251953125, -0.09912109375, -0.353515625, -0.1484375, 0.1484375, -0.035400390625, -0.052490234375, 0.0927734375, -0.142578125, -0.01483154296875, 0.0164794921875, 0.037109375, 0.1884765625, -0.03955078125, -0.057861328125, 0.267578125, 0.10693359375, -0.04345703125, 0.0673828125, -0.00177764892578125, 0.1328125, -0.1630859375, -0.05908203125, -0.2255859375, 0.1220703125, 0.10791015625, -0.1943359375, -0.162109375, -0.142578125, 0.09033203125, -0.146484375, -0.12109375, 0.099609375, 0.263671875, 0.126953125, 0.140625, 0.11083984375, 0.0269775390625, -0.016357421875, 0.0029296875, 0.1474609375, -0.0654296875, -0.1669921875, 0.03662109375, 0.1494140625, -0.1474609375, 0.068359375, -0.09228515625, 0.1220703125, -0.091796875, 0.0908203125, -0.384765625, 0.030517578125, -0.216796875, -0.1259765625, -0.08642578125, -0.26171875, -0.0849609375, -0.1396484375, -0.0283203125, -0.203125, 0.291015625, -0.1357421875, -0.072265625, 0.1630859375, -0.1904296875, 0.22265625, 0.0556640625, 0.212890625, 0.050537109375, -0.09814453125, 0.12158203125, 0.010009765625, 0.15234375, -0.0223388671875, 0.0732421875, -0.271484375, -0.019775390625, -0.0712890625, 0.1396484375, -0.0654296875, 0.0888671875, 0.0145263671875, 0.033447265625, 0.12158203125, -0.0703125, -0.0228271484375, -0.1533203125, 0.22265625, -0.22265625, -0.181640625, 0.08642578125, -0.263671875, -0.037353515625, 0.043212890625, 0.0303955078125, -0.1904296875, 0.07080078125, 0.376953125, -0.01318359375, -0.0252685546875, -0.00775146484375, 0.0286865234375, -0.003509521484375, -0.08203125, 0.15234375, -0.2099609375, 0.16796875, 0.05322265625, 0.134765625, 0.051025390625, 0.004547119140625, 0.02099609375, 0.37890625, 0.2470703125, -0.255859375, -0.271484375, 0.040283203125, -0.00604248046875, -0.11865234375, -0.0595703125, 0.1455078125, -0.265625, -0.478515625, 0.027099609375, 0.038330078125, 0.0244140625, -0.1640625, -0.19921875, -0.08984375, -0.16015625, -0.060791015625, -0.034912109375, 0.2109375, 0.036376953125, 0.208984375, -0.005889892578125, 0.015625, 0.10986328125, 0.08154296875, 0.11328125, 0.296875, 0.306640625, -0.12890625, -0.1455078125, 0.11474609375, 0.0225830078125, -0.029052734375, -0.08544921875, -0.06201171875, -0.08740234375, -0.2177734375, 0.10986328125, 0.1455078125, 0.216796875, 0.07177734375, 0.02294921875, 0.1396484375, -0.1591796875, 0.22265625, -0.03515625, -0.03515625, 0.07763671875, -0.126953125, 0.25, -0.2470703125, 0.04345703125, 0.057861328125, -0.1171875, -0.1142578125, -0.294921875, 0.115234375, -0.169921875, -0.05078125, -0.09228515625, 0.041015625, -0.3984375, -0.2470703125, -0.1923828125, 0.4375, 0.1298828125, 0.12890625, 0.03271484375, -0.2255859375, 0.2080078125, 0.03564453125, -0.06982421875, 0.1279296875, -0.0030059814453125, 0.1728515625, 0.1572265625, 0.12890625, -0.162109375, -0.2412109375, -0.3984375, 0.0224609375, -0.061767578125, 0.1279296875, 0.0252685546875, 0.0186767578125, -0.0174560546875, -0.1572265625, -0.06201171875, -0.224609375, -0.0208740234375, 0.001922607421875, -0.173828125, 0.09521484375, 0.23046875, -0.169921875, -0.2236328125, 0.236328125, 0.05126953125, 0.138671875, -0.00238037109375, -0.056640625, -0.2890625, -0.11767578125, -0.11767578125, -0.287109375, -0.032470703125, 0.04150390625, -0.08447265625, 0.1953125, -0.2412109375, -0.1943359375, -0.044189453125, 0.055908203125, 0.012939453125, 0.054443359375, 0.1298828125, -0.0250244140625, 0.197265625, -0.1767578125, -0.15234375, 0.21484375, 0.005157470703125, 0.0126953125, 0.111328125, -0.0272216796875, 0.15625, -0.2080078125, -0.024169921875, -0.1162109375, -0.087890625, -0.020751953125, 0.017822265625, 0.00439453125, 0.1826171875, -0.1943359375, 0.263671875, 0.044677734375, 0.012939453125, 0.1591796875, 0.0859375, 0.1298828125, 0.0014190673828125, 0.0244140625, 0.259765625, 0.283203125, 0.11669921875, -0.03564453125, -0.25, -0.055908203125, 0.29296875, 0.04833984375, -0.033935546875, 0.1376953125, -0.19140625, 0.21875, -0.0830078125, -0.1005859375, 0.201171875, -0.1142578125, -0.005950927734375, -0.06982421875, -0.046875, -0.06591796875, 0.0245361328125, -0.271484375, 0.1962890625, 0.22265625, 0.0242919921875, 0.0439453125, 0.1025390625, -0.060791015625, 0.1181640625, -0.064453125, 0.11376953125, -0.1728515625, 0.171875, -0.1943359375, 0.0081787109375, 0.068359375, -0.1416015625]\n"
     ]
    }
   ],
   "source": [
    "print(concat_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Representing the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to represent the test data in the same way as the train data. So also when testing we need to create an array with the same 312 features. We first use SpaCy again to get the linguistics features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'part-of-speech': 'PRON', 'lemma': '-PRON-'}, {'part-of-speech': 'VERB', 'lemma': 'love'}, {'part-of-speech': 'NOUN', 'lemma': 'beer'}, {'part-of-speech': 'ADP', 'lemma': 'from'}, {'part-of-speech': 'PROPN', 'lemma': 'Munich'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence='I love beer from Munich'\n",
    "test_doc=nlp(test_sentence)\n",
    "\n",
    "test_instances=[]\n",
    "for token in test_doc:\n",
    "    one_test_instance={'part-of-speech': token.pos_, 'lemma': token.lemma_} # this concatenates the PoS and Lemma\n",
    "    test_instances.append(one_test_instance)\n",
    "\n",
    "print(test_instances)\n",
    "the_test_array = vec.fit_transform(test_instances).toarray()\n",
    "the_test_array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(the_test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem. The training and test array do not have the same number of columns. For the training set we had 12 features, and now we have 10. The vectorizer takes the properties and values from the data. Since the training and test data are different also the vector representation are different. Not only in size but also mixing positions and values differently. To fix this, we need to apply the vectorizer function to both the train and test data to create the array with the dimensions and after that split the data again.\n",
    "This is how we do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Harmonizing one-hot-vectors across training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'part-of-speech': 'PROPN', 'lemma': 'Germany'}, {'part-of-speech': 'PART', 'lemma': \"'s\"}, {'part-of-speech': 'NOUN', 'lemma': 'representative'}, {'part-of-speech': 'ADP', 'lemma': 'to'}, {'part-of-speech': 'DET', 'lemma': 'the'}, {'part-of-speech': 'PROPN', 'lemma': 'European'}, {'part-of-speech': 'PROPN', 'lemma': 'Union'}, {'part-of-speech': 'PRON', 'lemma': '-PRON-'}, {'part-of-speech': 'VERB', 'lemma': 'love'}, {'part-of-speech': 'NOUN', 'lemma': 'beer'}, {'part-of-speech': 'ADP', 'lemma': 'from'}, {'part-of-speech': 'PROPN', 'lemma': 'Munich'}]\n"
     ]
    }
   ],
   "source": [
    "vec = DictVectorizer()\n",
    "## First we concatenate the training and test instances and fit these to a vector representation\n",
    "train_and_test_instance = training_instances + test_instances\n",
    "print(train_and_test_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 19)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_array = vec.fit_transform(train_and_test_instance).toarray()\n",
    "the_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we now have 12 rows (tokens) and 19 values. From this shared feature space, we need to recover the data corresponding to the training data and the data corresponding to the test data. Since the order is based on the concatenation, we can take the length of the training_instances to separate the first part as the training data and the second part as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(the_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training words = (7, 19)\n",
      "Number of test words = (5, 19)\n"
     ]
    }
   ],
   "source": [
    "# For the training set we take the fiorst part of the data upto the length of the training_instances\n",
    "training_onehot = the_array[:len(training_instances)]\n",
    "#For the test set, we take the remaining part of the data starting at the length of the training_instances\n",
    "#(remember that '0' is the first data element)\n",
    "test_onehot = the_array[len(training_instances):]\n",
    "\n",
    "print('Number of training words =', training_onehot.shape)\n",
    "print('Number of test words =', test_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(training_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(test_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we ensured that the feature space is the same for the training and test data. Next we get the embeddings for both sets and combine these with the one-got-vector representations. We start with the training data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training_input=np.array(training_onehot)\n",
    "embeddings_training_input=np.array(training_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7, 319)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words=training_onehot.shape[0]\n",
    "concat_train_input=[]\n",
    "for index in range(num_words):\n",
    "    print(index)\n",
    "    representation=list(training_onehot[index]) + list(embeddings_training_input[index]) # concatenate features per word\n",
    "    concat_train_input.append(representation)\n",
    "\n",
    "# we check the shape\n",
    "np.array(concat_train_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.259765625, 0.140625, 0.2470703125, 0.00958251953125, -0.25, -0.08251953125, -0.09912109375, -0.353515625, -0.1484375, 0.1484375, -0.035400390625, -0.052490234375, 0.0927734375, -0.142578125, -0.01483154296875, 0.0164794921875, 0.037109375, 0.1884765625, -0.03955078125, -0.057861328125, 0.267578125, 0.10693359375, -0.04345703125, 0.0673828125, -0.00177764892578125, 0.1328125, -0.1630859375, -0.05908203125, -0.2255859375, 0.1220703125, 0.10791015625, -0.1943359375, -0.162109375, -0.142578125, 0.09033203125, -0.146484375, -0.12109375, 0.099609375, 0.263671875, 0.126953125, 0.140625, 0.11083984375, 0.0269775390625, -0.016357421875, 0.0029296875, 0.1474609375, -0.0654296875, -0.1669921875, 0.03662109375, 0.1494140625, -0.1474609375, 0.068359375, -0.09228515625, 0.1220703125, -0.091796875, 0.0908203125, -0.384765625, 0.030517578125, -0.216796875, -0.1259765625, -0.08642578125, -0.26171875, -0.0849609375, -0.1396484375, -0.0283203125, -0.203125, 0.291015625, -0.1357421875, -0.072265625, 0.1630859375, -0.1904296875, 0.22265625, 0.0556640625, 0.212890625, 0.050537109375, -0.09814453125, 0.12158203125, 0.010009765625, 0.15234375, -0.0223388671875, 0.0732421875, -0.271484375, -0.019775390625, -0.0712890625, 0.1396484375, -0.0654296875, 0.0888671875, 0.0145263671875, 0.033447265625, 0.12158203125, -0.0703125, -0.0228271484375, -0.1533203125, 0.22265625, -0.22265625, -0.181640625, 0.08642578125, -0.263671875, -0.037353515625, 0.043212890625, 0.0303955078125, -0.1904296875, 0.07080078125, 0.376953125, -0.01318359375, -0.0252685546875, -0.00775146484375, 0.0286865234375, -0.003509521484375, -0.08203125, 0.15234375, -0.2099609375, 0.16796875, 0.05322265625, 0.134765625, 0.051025390625, 0.004547119140625, 0.02099609375, 0.37890625, 0.2470703125, -0.255859375, -0.271484375, 0.040283203125, -0.00604248046875, -0.11865234375, -0.0595703125, 0.1455078125, -0.265625, -0.478515625, 0.027099609375, 0.038330078125, 0.0244140625, -0.1640625, -0.19921875, -0.08984375, -0.16015625, -0.060791015625, -0.034912109375, 0.2109375, 0.036376953125, 0.208984375, -0.005889892578125, 0.015625, 0.10986328125, 0.08154296875, 0.11328125, 0.296875, 0.306640625, -0.12890625, -0.1455078125, 0.11474609375, 0.0225830078125, -0.029052734375, -0.08544921875, -0.06201171875, -0.08740234375, -0.2177734375, 0.10986328125, 0.1455078125, 0.216796875, 0.07177734375, 0.02294921875, 0.1396484375, -0.1591796875, 0.22265625, -0.03515625, -0.03515625, 0.07763671875, -0.126953125, 0.25, -0.2470703125, 0.04345703125, 0.057861328125, -0.1171875, -0.1142578125, -0.294921875, 0.115234375, -0.169921875, -0.05078125, -0.09228515625, 0.041015625, -0.3984375, -0.2470703125, -0.1923828125, 0.4375, 0.1298828125, 0.12890625, 0.03271484375, -0.2255859375, 0.2080078125, 0.03564453125, -0.06982421875, 0.1279296875, -0.0030059814453125, 0.1728515625, 0.1572265625, 0.12890625, -0.162109375, -0.2412109375, -0.3984375, 0.0224609375, -0.061767578125, 0.1279296875, 0.0252685546875, 0.0186767578125, -0.0174560546875, -0.1572265625, -0.06201171875, -0.224609375, -0.0208740234375, 0.001922607421875, -0.173828125, 0.09521484375, 0.23046875, -0.169921875, -0.2236328125, 0.236328125, 0.05126953125, 0.138671875, -0.00238037109375, -0.056640625, -0.2890625, -0.11767578125, -0.11767578125, -0.287109375, -0.032470703125, 0.04150390625, -0.08447265625, 0.1953125, -0.2412109375, -0.1943359375, -0.044189453125, 0.055908203125, 0.012939453125, 0.054443359375, 0.1298828125, -0.0250244140625, 0.197265625, -0.1767578125, -0.15234375, 0.21484375, 0.005157470703125, 0.0126953125, 0.111328125, -0.0272216796875, 0.15625, -0.2080078125, -0.024169921875, -0.1162109375, -0.087890625, -0.020751953125, 0.017822265625, 0.00439453125, 0.1826171875, -0.1943359375, 0.263671875, 0.044677734375, 0.012939453125, 0.1591796875, 0.0859375, 0.1298828125, 0.0014190673828125, 0.0244140625, 0.259765625, 0.283203125, 0.11669921875, -0.03564453125, -0.25, -0.055908203125, 0.29296875, 0.04833984375, -0.033935546875, 0.1376953125, -0.19140625, 0.21875, -0.0830078125, -0.1005859375, 0.201171875, -0.1142578125, -0.005950927734375, -0.06982421875, -0.046875, -0.06591796875, 0.0245361328125, -0.271484375, 0.1962890625, 0.22265625, 0.0242919921875, 0.0439453125, 0.1025390625, -0.060791015625, 0.1181640625, -0.064453125, 0.11376953125, -0.1728515625, 0.171875, -0.1943359375, 0.0081787109375, 0.068359375, -0.1416015625]\n"
     ]
    }
   ],
   "source": [
    "print(concat_train_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_input=np.array(test_onehot)\n",
    "embeddings_test_input=np.array(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 319)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words=test_onehot.shape[0]\n",
    "concat_test_input=[]\n",
    "for index in range(num_words):\n",
    "    print(index)\n",
    "    representation=list(test_onehot[index]) + list(embeddings_test_input[index]) # concatenate features per word\n",
    "    concat_test_input.append(representation)\n",
    "\n",
    "# we check the shape\n",
    "np.array(concat_test_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the classifier in the same way as we did before but now with the concatenated features, where the one-hot-vectors are aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf.fit(concat_train_input, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'O' 'O' 'O' 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "pred=lin_clf.predict(concat_test_input)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NERC datasets\n",
    "\n",
    "Now that we've seen how to represent linguistic features, we also need to access real linguistic training data for the NERC task. In this section, we will look at large data sets that have been created by the community in which people have been annotating entities. In the assignment, you will use this data to train and test models that give a realistic performance.\n",
    "\n",
    "Here, we will load two NERC datasets and quickly inspect their contents.\n",
    "\n",
    "**Preparation** Please download the .zip file with the two datasets from [this link](http://kyoto.let.vu.nl/~vossen/rma_hlt/nerc_datasets.zip)\n",
    "\n",
    "Then unpack the .zip, so that the folder `nerc_datasets` is created in the same directory as this notebook. If you want to store it elsewhere, you can do that but need to adapt the path in the calls below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 CoNLL-2003\n",
    "\n",
    " One of the most popular datasets is [CoNLL-2003](http://aclweb.org/anthology/W03-0419), which was provided with the zip file you just downloaded. You can open the file \"train.txt\" in a text editor to inspect its content:\n",
    "\n",
    "````\n",
    "-DOCSTART- -X- -X- O\n",
    "\n",
    "EU NNP B-NP B-ORG\n",
    "rejects VBZ B-VP O\n",
    "German JJ B-NP B-MISC\n",
    "call NN I-NP O\n",
    "to TO B-VP O\n",
    "boycott VB I-VP O\n",
    "British JJ B-NP B-MISC\n",
    "lamb NN I-NP O\n",
    ". . O O\n",
    "\n",
    "Peter NNP B-NP B-PER\n",
    "Blackburn NNP I-NP I-PER\n",
    "\n",
    "BRUSSELS NNP B-NP B-LOC\n",
    "1996-08-22 CD I-NP O\n",
    "````\n",
    "\n",
    "It follows the IOB format with one token on a line followed by columns wit the PoS, the constituent and the IOB entity tag. You can check the \"test.txt\" file to see it has a similar format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load it using the following code snippet, which makes use of the NLTK function ConllCorpusReader to do the magic. More information on the ConllCorpusReader can be found here: https://www.nltk.org/_modules/nltk/corpus/reader/conll.html\n",
    "\n",
    "The function has three parameters:\n",
    "\n",
    "* the path to the folder where ConLL-2003 is stored (locally in my case)\n",
    "* the name of the file that will be loaded from that folder\n",
    "* labels for the columns that are expected in the input file\n",
    "\n",
    "We store the result in a variable with the name 'train' which is of the type 'nltk.corpus.reader.conll.ConllCorpusReader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "train = ConllCorpusReader('nerc_datasets/CONLL2003',\n",
    "                          'train.txt', # this will load the file 'train.txt', for the exercise you also need to load 'test.xt' \n",
    "                          ['words', 'pos', 'ignore', 'chunk'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use 'dir' to see it has many data elements that correspond to the many different features that can be found in the CoNNL data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHUNK',\n",
       " 'COLUMN_TYPES',\n",
       " 'IGNORE',\n",
       " 'NE',\n",
       " 'POS',\n",
       " 'SRL',\n",
       " 'TREE',\n",
       " 'WORDS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_chunk_types',\n",
       " '_colmap',\n",
       " '_encoding',\n",
       " '_fileids',\n",
       " '_get_chunked_words',\n",
       " '_get_column',\n",
       " '_get_iob_words',\n",
       " '_get_parsed_sent',\n",
       " '_get_root',\n",
       " '_get_srl_instances',\n",
       " '_get_srl_spans',\n",
       " '_get_tagged_words',\n",
       " '_get_words',\n",
       " '_grids',\n",
       " '_pos_in_tree',\n",
       " '_read_grid_block',\n",
       " '_require',\n",
       " '_root',\n",
       " '_root_label',\n",
       " '_srl_includes_roleset',\n",
       " '_tagset',\n",
       " '_tree_class',\n",
       " 'abspath',\n",
       " 'abspaths',\n",
       " 'chunked_sents',\n",
       " 'chunked_words',\n",
       " 'citation',\n",
       " 'encoding',\n",
       " 'ensure_loaded',\n",
       " 'fileids',\n",
       " 'iob_sents',\n",
       " 'iob_words',\n",
       " 'license',\n",
       " 'open',\n",
       " 'parsed_sents',\n",
       " 'raw',\n",
       " 'readme',\n",
       " 'root',\n",
       " 'sents',\n",
       " 'sep',\n",
       " 'srl_instances',\n",
       " 'srl_spans',\n",
       " 'tagged_sents',\n",
       " 'tagged_words',\n",
       " 'unicode_repr',\n",
       " 'words']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are for now only interested in the token, the pos and the ne_label. Let's check the first one in train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU NNP B-ORG\n"
     ]
    }
   ],
   "source": [
    "for token, pos, ne_label in train.iob_words():\n",
    "    print(token, pos, ne_label) # please represent this information using a dictionary for the feature representation\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can for example iterate through this data, and make a list of the tokens as inputs, and of the `ne_label` values as desirable outputs. The input tokens could for example be looked up in our word embeddings dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vectors=[]\n",
    "labels=[]\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        input_vectors.append(vector)\n",
    "        labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully loaded our data. Let's see how many tokens/labels we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ten labels = ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER']\n"
     ]
    }
   ],
   "source": [
    "print('Last ten labels =', labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we should have the same size of input_vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621\n"
     ]
    }
   ],
   "source": [
    "print(len(input_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, we could easily train a model on this data as shown in above by combining the input vectors with the labels in a fit function. You will see it takes a lot longer to train the classifier with this  data set that has over 200K instances. On my machine it took about 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf.fit(input_vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to apply this classifier to a data set for testing, you need to apply the same vectorization procedure as you have followed for the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you apply a classifier to a data set, it is important to know the data set and especially the statistics about how the labels are distributed. In other words, how often do tokens in the data set belong a human annotated data set?\n",
    "\n",
    "This tells you how frequent or rare certain data categories are and how challenging it is for a system to learn and predict each category.\n",
    "\n",
    "Because we have created a list of labels from our data, we can use a simple Python function *Counter* to get the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 169578, 'B-LOC': 7140, 'B-PER': 6600, 'B-ORG': 6321, 'I-PER': 4528, 'I-ORG': 3704, 'B-MISC': 3438, 'I-LOC': 1157, 'I-MISC': 1155})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "print(Counter(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly shows that most tokens get the label *O* and the actually enity tokens range between 1155 and 7140."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Kaggle\n",
    "[*Kaggle*](https://www.kaggle.com/docs) is an open source platform for sharing data and competitions. It has over 1000's of datasets and  frequently releases new data and challenges. We are going to have a quick look at the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus) that they provided and which was also provided in the zip file you downloaded as a so-called CSV file: ner.csv and ner_v2.csv. CSV stands for comma-separated-values and it is a commonly used format to exchange e.g. Excell or spreadsheet data as text files. Instances of data are represented on separate lines followed by values separated by commas. Another format is tab-separated-values or TSV, in which case tabs are used as in the CoNLL formats. Very often people store TSV formats in files with the extension \".csv\", so it is always good practice to check the actual content to see what is used as a separator. The first line of a CSV or TSV file is usually the header that labels the different columns. \n",
    "\n",
    "The [*pandas*](https://pandas.pydata.org) package is a powerful package to handle data in various formats. You can check the website for details and documentation. Here we are going to use it to inspect the data.\n",
    "\n",
    "To load data fYou can load it in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'nerc_datasets/kaggle/ner_v2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the following output after running the above code cell:\n",
    "```\n",
    "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n",
    "```\n",
    "You can ignore this.\n",
    "\n",
    "**pandas.read_csv** will load the csv file into a [pandas DataFrame](https://towardsdatascience.com/pandas-dataframe-a-lightweight-intro-680e3a212b96)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect which columns are in the csv file by running the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos',\n",
       "       'next-next-shape', 'next-next-word', 'next-pos', 'next-shape',\n",
       "       'next-word', 'pos', 'prev-iob', 'prev-lemma', 'prev-pos',\n",
       "       'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape',\n",
       "       'prev-prev-word', 'prev-shape', 'prev-word', 'sentence_idx', 'shape',\n",
       "       'word', 'tag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can seen that a wide range of features is given for each token. [Here](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus), you can read what each column represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You loop can loop through the dataset in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "id                             0\n",
      "lemma                   thousand\n",
      "next-lemma                    of\n",
      "next-next-lemma         demonstr\n",
      "next-next-pos                NNS\n",
      "next-next-shape        lowercase\n",
      "next-next-word     demonstrators\n",
      "next-pos                      IN\n",
      "next-shape             lowercase\n",
      "next-word                     of\n",
      "pos                          NNS\n",
      "prev-iob              __START1__\n",
      "prev-lemma            __start1__\n",
      "prev-pos              __START1__\n",
      "prev-prev-iob         __START2__\n",
      "prev-prev-lemma       __start2__\n",
      "prev-prev-pos         __START2__\n",
      "prev-prev-shape         wildcard\n",
      "prev-prev-word        __START2__\n",
      "prev-shape              wildcard\n",
      "prev-word             __START1__\n",
      "sentence_idx                   1\n",
      "shape                capitalized\n",
      "word                   Thousands\n",
      "tag                            O\n",
      "Name: 0, dtype: object\n",
      "NERC label O\n"
     ]
    }
   ],
   "source": [
    "for index, instance in kaggle_dataset.iterrows():\n",
    "    print()\n",
    "    print(index)\n",
    "    print(instance) # you can access information by using instance['A COLUMN NAME'] which you can use to convert to a dictionary needed for the feature representation.\n",
    "    print('NERC label', instance['tag'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that each token has many different features that people have considered useful for trhe task of NERC. In addition to the usual suspects that we saw before, each token also has features indicating previous and next words and their PoS, but als the shape of the word (upper and lower case patterns), and even the previous IOB tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use all these features as inputs in a machine learning model with our DictVectorizer, or by transforming them using embeddings if the values are words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
