{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab6.1-Topic Modeling Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "\n",
    "Credits: this notebook is an adaptation of a blog by Shivam Bansal:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab provides a basic introduction into topic modeling and classification. It consists of the following notebooks:\n",
    "\n",
    "* Lab6.1-Topic-modeling-introduction.ipynb (this notebook)\n",
    "* Lab6.2-Topic-modeling-gensim.ipynb (how to create and apply topic models using gensim)\n",
    "* Lab6.3-Topic-modeling-sklearn.ipynb (how to create topic models using sklearn) + BERT topic modeling\n",
    "* Lab6.4-Topic-classification-BERT.ipynb (how to fine-tune transformer models for topic classification)\n",
    "* Lab6-assignment-topic-classification.ipynb\n",
    "\n",
    "We suggest you work through these notebooks in this order. In this notebook, we explain some of the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a clustering task that groups documents on the basis of their topic. A topic can be defined as a main area of interest that the text is about. There is no a priori definition of all the topics. Topic modelling typically assumes that any set of documents can be split into groups or clusters that use similar words. \n",
    "\n",
    "An example of topic modeling done at the VU for the National Science Agenda can be see here:\n",
    "\n",
    "http://i.amcat.nl/renwa/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many approaches for obtaining topics from a text such as – Term Frequency and Inverse Document Frequency, and NonNegative Matrix Factorization techniques. Latent Dirichlet Allocation (LDA) is the most popular topic modeling technique.\n",
    "\n",
    "LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a matrix factorization technique. In vector space, any corpus (collection of documents) can be represented as a document-term matrix. The following matrix shows a corpus of N documents D1, D2, D3 … Dn and vocabulary size of M words W1,W2 .. Wn. The value of i,j cell gives the frequency count of word Wj in Document Di.\n",
    "\n",
    "<img src=\"images/lda2.1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA converts this Document-Term Matrix into two lower dimensional matrices – M1 and M2.\n",
    "M1 is a document-topics matrix and M2 is a topic – terms matrix with dimensions (N,  K) and (K, M) respectively, where N is the number of documents, K is the number of topics and M is the vocabulary size.\n",
    "\n",
    "<img src=\"images/lda2.2.png\">\n",
    "\n",
    "<img src=\"images/lda2.3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these two matrices already provides topic word and document topic distributions. However, these distribution needs to be improved, which is the main aim of LDA. LDA makes use of sampling techniques in order to improve these matrices.\n",
    "\n",
    "It Iterates through each word “w” for each document “d” and tries to adjust the current topic – word assignment with a new assignment. A new topic “k” is assigned to word “w” with a probability P which is a product of two probabilities p1 and p2.\n",
    "\n",
    "For every topic, two probabilities p1 and p2 are calculated. P1 – p(topic t / document d) = the proportion of words in document d that are currently assigned to topic t. P2 – p(word w / topic t) = the proportion of assignments to topic t over all documents that come from this word w.\n",
    "\n",
    "The current topic – word assignment is updated with a new topic with the probability, product of p1 and p2 . In this step, the model assumes that all the existing word – topic assignments except the current word are correct. This is essentially the probability that topic t generated word w, so it makes sense to adjust the current word’s topic with new probability.\n",
    "\n",
    "After a number of iterations, a steady state is achieved where the document topic and topic term distributions are fairly good. This is the convergence point of LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of LDA\n",
    "\n",
    "Alpha and Beta Hyperparameters – alpha represents document-topic density and Beta represents topic-word density. Higher the value of alpha, documents are composed of more topics and lower the value of alpha, documents contain fewer topics. On the other hand, higher the beta, topics are composed of a large number of words in the corpus, and with the lower value of beta, they are composed of few words.\n",
    "\n",
    "Number of Topics – Number of topics to be extracted from the corpus. Researchers have developed approaches to obtain an optimal number of topics by using Kullback Leibler Divergence Score. I will not discuss this in detail, as it is too mathematical. For understanding, one can refer to this blog post: https://www.machinelearningplus.com/machine-learning/kl-divergence-what-is-it-and-mathematical-details-explained/\n",
    "\n",
    "Number of Topic Terms – Number of terms composed in a single topic. It is generally decided according to the requirement. If the problem statement talks about extracting themes or concepts, it is recommended to choose a higher number, if problem statement talks about extracting features or terms, a low number is recommended.\n",
    "\n",
    "Number of Iterations / passes – Maximum number of iterations allowed to LDA algorithm for convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
