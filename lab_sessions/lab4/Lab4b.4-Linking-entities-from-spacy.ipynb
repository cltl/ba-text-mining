{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB4b.4 Linking entities detected by Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import requests\n",
    "import urllib\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import urlencode\n",
    "import xml.etree.cElementTree as ET\n",
    "from lxml import etree\n",
    "import time\n",
    "import json\n",
    "# import our own utility functions and classes\n",
    "import lab4_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en') # other languages: de, es, pt, fr, it, nl\n",
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"But Google is starting from behind Europe in January. The company made a late push into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa software, which runs on its Echo and Dot devices, have clear leads in consumer adoption.\"\"\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But Google is starting from behind Europe in January. The company made a late push into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa software, which runs on its Echo and Dot devices, have clear leads in consumer adoption.\n"
     ]
    }
   ],
   "source": [
    "## Get all the text\n",
    "print(doc.doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google 1 2 4 10 ORG\n",
      "Europe 6 7 35 41 LOC\n",
      "January 8 9 45 52 DATE\n",
      "Apple’s Siri 20 23 102 114 ORG\n",
      "iPhones 26 27 129 136 ORG\n",
      "Amazon 29 30 142 148 ORG\n",
      "Echo and Dot 38 41 185 197 ORG\n"
     ]
    }
   ],
   "source": [
    "# Get all the entities with the basic information\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this basic information from spaCy on entities, we can now adapt our functions for AIDA and Spotlight to find the entity links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linking spaCy entities with AIDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### aida function to handle the entity mentions in a spaCy Doc object\n",
    "from spacy.tokens import Span\n",
    "def aida_disambiguation(doc, aida_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with AIDA.\n",
    "    \"\"\"\n",
    "    entities=[]\n",
    "    # AIDA expects entity mentions that are pre-marked inside text. \n",
    "    # For example, the sentence \"Obama visited Paris today\", \n",
    "    # should be transformed to \"[[Obama]] visited [[Paris]] today.\"\n",
    "    # We do this in the next 5 lines of code.\n",
    "    original_content = doc.text\n",
    "    #print(original_content)\n",
    "    new_content=\"\"\n",
    "    current_pos=0\n",
    "    for entity in doc.ents:\n",
    "        print(entity.text, entity.start_char, entity.end_char, entity.label_)\n",
    "        entity_span=original_content[entity.start_char: entity.end_char]\n",
    "        #print(entity_span)\n",
    "        new_content+=original_content[current_pos:entity.start_char] + '[[' + entity_span + ']]'\n",
    "        current_pos=entity.end_char\n",
    "\n",
    "    # Now, we can run the AIDA library with this string.\n",
    "    params={\"text\": new_content, \"tag_mode\": 'manual'}\n",
    "    request = Request(aida_url, urlencode(params).encode())\n",
    "    this_json = urlopen(request).read().decode('unicode-escape')\n",
    "    try:\n",
    "        results=json.loads(this_json)\n",
    "    except:\n",
    "        return doc  ### if we fail, we return the doc as is\n",
    "    \n",
    "    # Let's normalize the disambiguated entities.\n",
    "    # This means mostly removing the first part of the URI which is always the same (YAGO:)\n",
    "    # and leaving only the entity identification part (e.g., Barack_Obama).\n",
    "    \n",
    "    dis_entities={}\n",
    "    for dis_entity in results['mentions']:\n",
    "        if 'bestEntity' in dis_entity.keys():\n",
    "            best_entity=dis_entity['bestEntity']['kbIdentifier']\n",
    "            clean_url=best_entity[5:] #SKIP YAGO:\n",
    "        else:\n",
    "            clean_url='NIL'\n",
    "        dis_entities[str(dis_entity['offset'])] = clean_url # BECOMES THE VALUE IN THE DICTIONARY FOR THE OFFSET(REPRESENTING THE START OF THE MENTION) IN THE TEXT\n",
    "\n",
    "    # We can now store the entity to our class instance for later processing.\n",
    "    for entity in doc.ents:\n",
    "        start = entity.start_char\n",
    "        try:\n",
    "            dis_url = str(dis_entities[str(start)])  # WE GET THE DISAMBIGUATED URL\n",
    "        except:\n",
    "            dis_url='NIL'\n",
    "        #print(dis_url)\n",
    "        # Create a spaCy Span object for each entity and add it to our entity list \n",
    "        linked_entity = Span(doc, start=entity.start, end=entity.end, label=entity.label_, kb_id=dis_url)\n",
    "        entities.append(linked_entity)\n",
    "          \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google 4 10 ORG\n",
      "Europe 35 41 LOC\n",
      "January 45 52 DATE\n",
      "Apple’s Siri 102 114 ORG\n",
      "iPhones 129 136 ORG\n",
      "Amazon 142 148 ORG\n",
      "Echo and Dot 185 197 ORG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    }
   ],
   "source": [
    "# AIDA is running in an external location - for this reason, we need to send an HTTP request. This should take a few minutes.\n",
    "aida_disambiguation_url = \"https://gate.d5.mpi-inf.mpg.de/aida/service/disambiguate\"\n",
    "\n",
    "processed_aida=aida_disambiguation(doc, aida_disambiguation_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mention: Google; offset:4-10; type:ORG; url:Google\n",
      "mention: Europe; offset:35-41; type:LOC; url:Europe\n",
      "mention: January; offset:45-52; type:DATE; url:Deaths_in_January_2013\n",
      "mention: Apple’s Siri; offset:102-114; type:ORG; url:NIL\n",
      "mention: iPhones; offset:129-136; type:ORG; url:NIL\n",
      "mention: Amazon; offset:142-148; type:ORG; url:Amazon.com\n",
      "mention: Echo and Dot; offset:185-197; type:ORG; url:NIL\n"
     ]
    }
   ],
   "source": [
    "## We iterate over our entities from spaCy and print out the data with the URL\n",
    "\n",
    "for entity in processed_aida:\n",
    "    print('mention: %s; offset:%s-%s; type:%s; url:%s'% (entity.text, entity.start_char, entity.end_char, entity.label_, entity.kb_id_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linking spaCy entities with DBPedia Spotlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But Google is starting from behind Europe in January. The company made a late push into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa software, which runs on its Echo and Dot devices, have clear leads in consumer adoption.\n"
     ]
    }
   ],
   "source": [
    "## Get all the text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### spotlight function to handle the entity mentions in a spaCy Doc object\n",
    "\n",
    "def spotlight_disambiguate(doc, spotlight_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with DBpedia Spotlight.\n",
    "    \"\"\"\n",
    "\n",
    "    entities=[]\n",
    "   \n",
    "    # Similar as with AIDA, we first prepare the document text and the mentions\n",
    "    # in order to provide these to Spotlight as input.\n",
    "\n",
    "    # We build up the XML structure that Spotligh wants as input\n",
    "    # The next function Element creates the XML element with the text attribute\n",
    "    annotation = etree.Element(\"annotation\", text=doc.text)\n",
    "\n",
    "    # We iterate over the eneity mentions from our Reuters data to create the surface form elements\n",
    "    for entity in doc.ents:\n",
    "        sf = etree.SubElement(annotation, \"surfaceForm\")\n",
    "        sf.set(\"name\", entity.text)\n",
    "        sf.set(\"offset\", str(entity.start_char))\n",
    "    my_xml=etree.tostring(annotation, xml_declaration=True, encoding='UTF-8')\n",
    "    # Send a disambiguation request to spotlight\n",
    "    results=requests.post(spotlight_url, urllib.parse.urlencode({'text':my_xml, 'confidence': 0.5}), \n",
    "                          headers={'Accept': 'application/json'})\n",
    "    # Note that you can adjust the confidence value. Check the online demo to see the effect. \n",
    "    # What will happen with the recall and precision if you increase the confidence?\n",
    "\n",
    "    # Process the results and normalize the entity URIs\n",
    "    j=results.json()\n",
    "    dis_entities={}\n",
    "    if 'Resources' in j: \n",
    "        resources=j['Resources']\n",
    "    else: \n",
    "        resources=[]\n",
    "    for dis_entity in resources:\n",
    "        dis_entities[str(dis_entity['@offset'])] = utils.normalizeURL(dis_entity['@URI'])\n",
    "\n",
    "    # Let's now store the URLs by Spotlight to our class for later analysis.\n",
    "    for entity in doc.ents:\n",
    "        start = entity.start_char\n",
    "        if str(start) in dis_entities:\n",
    "            dis_url = dis_entities[str(start)]\n",
    "        else:\n",
    "            dis_url = 'NIL'\n",
    "        print(dis_url)\n",
    "        linked_entity = Span(doc, start=entity.start, end=entity.end, label=entity.label_, kb_id=dis_url)\n",
    "        entities.append(linked_entity)\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google\n",
      "Europe\n",
      "January\n",
      "Siri\n",
      "IPhone\n",
      "Amazon_Video\n",
      "NIL\n"
     ]
    }
   ],
   "source": [
    "spotlight_disambiguation_url=\"http://model.dbpedia-spotlight.org/en/disambiguate\"\n",
    "\n",
    "processed_spotlight=spotlight_disambiguate(doc, spotlight_disambiguation_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mention: Google; offset:4-10; type:ORG; url:Google\n",
      "mention: Europe; offset:35-41; type:LOC; url:Europe\n",
      "mention: January; offset:45-52; type:DATE; url:Deaths_in_January_2013\n",
      "mention: Apple’s Siri; offset:102-114; type:ORG; url:NIL\n",
      "mention: iPhones; offset:129-136; type:ORG; url:NIL\n",
      "mention: Amazon; offset:142-148; type:ORG; url:Amazon.com\n",
      "mention: Echo and Dot; offset:185-197; type:ORG; url:NIL\n"
     ]
    }
   ],
   "source": [
    "for entity in processed_spotlight:\n",
    "    print('mention: %s; offset:%s-%s; type:%s; url:%s'% (entity.text, entity.start_char, entity.end_char, entity.label_, entity.kb_id_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
